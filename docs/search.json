[
  {
    "objectID": "data.table.html",
    "href": "data.table.html",
    "title": "The data.table R package",
    "section": "",
    "text": "For most cases, the most proper data wrangling tool in R is the dplyr package. Nonetheless, when dealing with large amounts of data, the data.table package is the fastest alternative available for doing data processing within R (see the benchmarks).\nForeSITE"
  },
  {
    "objectID": "data.table.html#reading-and-writing-data",
    "href": "data.table.html#reading-and-writing-data",
    "title": "The data.table R package",
    "section": "Reading and Writing Data",
    "text": "Reading and Writing Data\nReading and writing operations with data.table’s fread and fwrite are highly optimized. Here is a benchmark we can do on our own:\n\nlibrary(readr) # tidyverse's\nlibrary(data.table)\nlibrary(microbenchmark)\n\n# Generating a large dataset of random numbers\nset.seed(1231)\nx &lt;- runif(5e4 * 10) |&gt; matrix(ncol = 10) |&gt; data.frame()\n\n# Creating tempfiles\ntemp_dt &lt;- tempfile(fileext = \".csv\")\ntemp_tv &lt;- tempfile(fileext = \".csv\")\ntemp_r  &lt;- tempfile(fileext = \".csv\")\n\n\nbm &lt;- microbenchmark(\n  readr      = write_csv(x, temp_tv, num_threads = 1L, progress = FALSE),\n  data.table = fwrite(\n    x, temp_dt, verbose = FALSE, nThread = 1L,\n    showProgress = FALSE),\n  base       = write.csv(x, temp_r),\n  times = 20\n)\n\nWarning in microbenchmark(readr = write_csv(x, temp_tv, num_threads = 1L, :\nless accurate nanosecond times to avoid potential integer overflows\n\nbm\n\nUnit: milliseconds\n       expr       min        lq      mean    median        uq      max neval\n      readr  20.50656  20.85293  32.74184  22.92738  28.40509 192.2000    20\n data.table  21.73935  23.39716  25.49071  24.40361  25.86483  32.4754    20\n       base 219.56336 230.62471 239.80889 240.37378 247.95769 260.8928    20\n\n# We can also visualize it\nbm |&gt;\n  plot(\n    log = \"y\",\n    ylab = \"Time (ms) (log10-scale)\",\n    main = \"CSV Writing Benchmark\"\n  )\n\n\n\n\n\n\n\n\nThe same thing applies when reading data\n\n# Writing the data\nfwrite(x, temp_r, verbose = FALSE, nThread = 1L, showProgress = FALSE)\n\n# Benchmarking\nbm &lt;- microbenchmark(\n  readr      = read_csv(\n    temp_r, progress = FALSE, num_threads = 1L,\n    show_col_types = FALSE\n    ),\n  data.table = fread(\n    temp_r, verbose = FALSE, nThread = 1L,\n    showProgress = FALSE\n    ),\n  base       = read.csv(temp_r),\n  times = 10\n)\n\nWarning in microbenchmark(readr = read_csv(temp_r, progress = FALSE,\nnum_threads = 1L, : less accurate nanosecond times to avoid potential integer\noverflows\n\nbm\n\nUnit: milliseconds\n       expr        min        lq      mean    median        uq      max neval\n      readr  37.631276  38.50851  46.13079  39.05629  40.32838 109.9365    10\n data.table   9.598879  10.13672  17.68502  10.47573  20.14621  55.4172    10\n       base 166.714323 191.63904 214.94111 205.02200 225.25822 337.7869    10\n\nbm |&gt;\n  plot(\n    log = \"y\",\n    ylab = \"Time (ms) (log10-scale)\",\n    main = \"CSV Reading Benchmark\"\n  )\n\n\n\n\n\n\n\n\nUnder the hood, the readr package uses the vroom package. Nonetheless, there are some operations (dealing with character data mostly), where the vroom package shines. Regardless, data.table is a perfect alternative as it goes beyond just reading/writing data."
  },
  {
    "objectID": "data.table.html#example-data-manipulation",
    "href": "data.table.html#example-data-manipulation",
    "title": "The data.table R package",
    "section": "Example data manipulation",
    "text": "Example data manipulation\ndata.table is also the fastest for data manipulation. Here are a couple of examples aggregating data with data table vs dplyr\n\ninput &lt;- if (file.exists(\"flights14.csv\")) {\n   \"flights14.csv\"\n} else {\n  \"https://raw.githubusercontent.com/Rdatatable/data.table/master/vignettes/flights14.csv\"\n}\n\n# Reading the data\nflights_dt &lt;- fread(input)\nflights_tb &lt;- read_csv(input, show_col_types = FALSE)\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:data.table':\n\n    between, first, last\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# To avoid some messaging from the function\noptions(dplyr.summarise.inform = FALSE)\n\nmicrobenchmark(\n  data.table = flights_dt[, .N, by = .(origin, dest)],\n  dplyr = flights_tb |&gt;\n    group_by(origin, dest) |&gt;\n    summarise(n = n())\n)\n\nUnit: milliseconds\n       expr      min       lq     mean   median       uq       max neval\n data.table 1.546356 1.668229 2.193217 1.774050 1.972735  5.053373   100\n      dplyr 4.719141 4.880312 5.634190 4.976457 5.245601 33.803516   100"
  },
  {
    "objectID": "parallel.html",
    "href": "parallel.html",
    "title": "Parallel computing with R",
    "section": "",
    "text": "Note\n\n\n\nThis content was originally published in the book “Applied HPC with R” by George G. Vega Yon, Ph.D. You can find the book at https://book-hpc.ggvy.cl\nWhile most people see R as a slow programming language, it has powerful features that dramatically accelerate your code 1. Although R wasn’t necessarily built for speed, there are some tools and ways in which we can accelerate R. This chapter introduces what we will understand as High-performance computing in R.\nForeSITE"
  },
  {
    "objectID": "parallel.html#high-performance-computing-an-overview",
    "href": "parallel.html#high-performance-computing-an-overview",
    "title": "Parallel computing with R",
    "section": "High-Performance Computing: An overview",
    "text": "High-Performance Computing: An overview\nFrom R’s perspective, we can think of HPC in terms of two or three things:2 Big data, parallel computing, and compiled code.\n\nBig Data\nWhen we talk about big data, we refer to cases where your computer struggles to handle a dataset. A typical example of the latter is when the number of observations (rows) in your data frame is too many to fit a linear regression model. Instead of buying a bigger computer, there are many good solutions to solve memory-related problems:\n\nOut-of-memory storage. The idea is simple, instead of using your RAM to load the data, use other methods to load the data. Two notewirthy alternatives are the bigmemory and implyr R packages. The bigmemory package provides methods for using “file-backed” matrices. On the other hand, implyr implements a wrapper to access Apache Impala, an SQL query engine for a cluster running Apache Hadoop.\nEfficient algorithms for big data: To avoid running out of memory with your regression analysis, the R packages biglm and biglasso deliver highly-efficient alternatives to glm and glmnet, respectively. Now, if your data fits your RAM, but you still struggle with data wrangling, the data.table package is the solution.\nStore it more efficiently: Finally, when it comes to linear algebra, the Matrix R package shines with its formal classes and methods for managing Sparse Matrices, i.e., big matrices whose entries are primarily zeros; for example, the dgCMatrix objects. Furthermore, Matrix comes shipped with R, which makes it even more appealing.\n\n\n\nParallel computing\n\n\n\n\n\nFlynn’s Classical Taxonomy (Blaise Barney, Introduction to Parallel Computing, Lawrence Livermore National Laboratory)\n\n\n\n\nWe will focus on the Single Instruction stream Multiple Data stream.\nIn general terms, a parallel computing program is one in which we use two or more computational threads simultaneously. Although computational thread usually means core, there are multiple levels at which a computer program can be parallelized. To understand this, we first need to see what composes a modern computer:\n\n\n\nSource: Original figure from LUMI consortium documentation [@lumi2023]\n\n\nStreaming SIMD Extensions [SSE] and Advanced Vector Extensions [AVX]\n\nSerial vs. Parallel\n\n\n\n\n\nSource: Blaise Barney, Introduction to Parallel Computing, Lawrence Livermore National Laboratory\n\n\n\n\nsource: Blaise Barney, Introduction to Parallel Computing, Lawrence Livermore National Laboratory\n\n\n\n\n\nHigh-performance computing in R\n\nSome vocabulary for HPC\nIn raw terms\n\nSupercomputer: A single big machine with thousands of cores/GPGPUs.\nHigh-Performance Computing (HPC): Multiple machines within a single network.\nHigh Throughput Computing (HTC): Multiple machines across multiple networks.\n\nYou may not have access to a supercomputer, but certainly, HPC/HTC clusters are more accessible these days, e.g., AWS provides a service to create HPC clusters at a low cost (allegedly, since nobody understands how pricing works)\n\n\n\nGPU vs. CPU\n\n\n\n\n\nNVIDIA Blog\n\n\n\n\n\nWhy use OpenMP if GPU is suited to compute-intensive operations? Well, mostly because OpenMP is VERY easy to implement (easier than CUDA, which is the easiest way to use GPU).3\n\n\n\nWhen is it a good idea?\n\n\n\n\n\nAsk yourself these questions before jumping into HPC!\n\n\n\n\n\n\nParallel computing in R\nWhile there are several alternatives (just take a look at the High-Performance Computing Task View), we’ll focus on the following R-packages for explicit parallelism:\n\nparallel: R package that provides ‘[s]upport for parallel computation, including random-number generation’.\nfuture: ‘[A] lightweight and unified Future API for sequential and parallel processing of R expression via futures.’\nRcpp + OpenMP: Rcpp is an R package for integrating R with C++ and OpenMP is a library for high-level parallelism for C/C++ and FORTRAN.\n\nOthers but not used here\n\nforeach for iterating through lists in parallel.\nRmpi for creating MPI clusters.\n\nAnd tools for implicit parallelism (out-of-the-box tools that allow the programmer not to worry about parallelization):\n\ngpuR for Matrix manipulation using GPU\ntensorflow an R interface to TensorFlow.\n\nA ton of other types of resources, notably the tools for working with batch schedulers such as Slurm, and HTCondor."
  },
  {
    "objectID": "parallel.html#footnotes",
    "href": "parallel.html#footnotes",
    "title": "Parallel computing with R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNonetheless, this claim can be said about almost any programming language; there are notable examples like the R package data.table [@datatable] which has been demonstrated to out-perform most data wrangling tools.↩︎\nMake sure to check out CRAN Task View on HPC.↩︎\nSadia National Laboratories started the Kokkos project, which provides a one-fits-all C++ library for parallel programming. More information on the Kokkos’s wiki site.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Attendees during the morning session led by Andrew Redd (Photo by George G Vega Yon)\nForeSITE"
  },
  {
    "objectID": "index.html#about-the-workshop",
    "href": "index.html#about-the-workshop",
    "title": "Welcome!",
    "section": "About the workshop",
    "text": "About the workshop\nEfficient programming is crucial for working with large datasets and complex analyses in R. This half-day workshop is designed to teach practical techniques for improving R code performance through vectorization, parallel computing, and specialized packages like data.table. Participants will learn how to identify bottlenecks in their code and apply appropriate optimization strategies to dramatically improve performance.\n\nTime and location\nThe workshop will take place on Friday August 15th, 2025 at 9am-1pm MT. More details will be provided to registered attendees.\n\n\nWhat are good use cases for efficient R programming?\n\nYou work with large datasets that take a long time to process\nYour R scripts run slowly and you need to optimize performance\nYou want to learn about vectorization techniques to replace slow loops\nYou’re interested in parallel computing to utilize multiple CPU cores\nYou want to master data.table for fast data manipulation\nYou need to process data that doesn’t fit in memory efficiently\nYou want to benchmark and profile your R code to identify bottlenecks\n\n\n\nWho should attend?\nR programming users with intermediate experience who want to improve their code’s performance.\n\n\nWhat level of programming should attendees have?\nAttendees should have solid experience using the R programming language, including writing functions, working with data frames, and basic data manipulation using tools like dplyr, base R, or data.table. Some familiarity with loops and apply functions would be helpful.\n\n\nWhat tools will be used during the workshop?\nBesides the R programming language, we will be using RStudio. Participants should have R and RStudio installed on their laptops. We will also cover specialized packages including:\n\ndata.table for fast data manipulation\nparallel for parallel computing\nmicrobenchmark and profvis for performance measurement"
  },
  {
    "objectID": "index.html#registration",
    "href": "index.html#registration",
    "title": "Welcome!",
    "section": "Registration",
    "text": "Registration\nFor registration, please reach out to your ForeSITE contact. You can also email us at george.vegayon@utah.edu."
  },
  {
    "objectID": "index.html#funding",
    "href": "index.html#funding",
    "title": "Welcome!",
    "section": "Funding",
    "text": "Funding\nThis workshop is organized by ForeSITE using funds from the CDC’s Center for Forecasting and Outbreak Analytics (CFA) (Award number 1U01CK000585; 75D30121F00003)."
  },
  {
    "objectID": "index.html#ai-disclaimer",
    "href": "index.html#ai-disclaimer",
    "title": "Welcome!",
    "section": "AI Disclaimer",
    "text": "AI Disclaimer\nThis project contains AI-generated content. Particularly, via assistance (code completion and suggestions) using GitHub Copilot."
  },
  {
    "objectID": "parallel_pkg.html",
    "href": "parallel_pkg.html",
    "title": "The parallel R package",
    "section": "",
    "text": "Note\n\n\n\nThis content was originally published in the book “Applied HPC with R” by George G. Vega Yon, Ph.D. You can find the book at https://book-hpc.ggvy.cl\nAlthough R was not built for parallel computing, multiple ways of parallelizing your R code exist. One of these is the parallel package. This R package, shipped with base R, provides various functions to parallelize R code using embarrassingly parallel computing, i.e., a divide-and-conquer-type strategy. The basic idea is to start multiple R sessions (usually called child processes), connect the main session with those, and send them instructions. This section goes over a common workflow to work with R’s parallel.\nForeSITE"
  },
  {
    "objectID": "parallel_pkg.html#parallel-workflow",
    "href": "parallel_pkg.html#parallel-workflow",
    "title": "The parallel R package",
    "section": "Parallel workflow",
    "text": "Parallel workflow\n(Usually) We do the following:\n\nCreate a PSOCK/FORK (or other) cluster using makePSOCKCluster/makeForkCluster (or makeCluster)\nCopy/prepare each R session (if you are using a PSOCK cluster):\n\nCopy objects with clusterExport\nPass expressions with clusterEvalQ\nSet a seed\n\nDo your call: parApply, parLapply, etc.\nStop the cluster with clusterStop"
  },
  {
    "objectID": "parallel_pkg.html#types-of-clusters-psock",
    "href": "parallel_pkg.html#types-of-clusters-psock",
    "title": "The parallel R package",
    "section": "Types of clusters: PSOCK",
    "text": "Types of clusters: PSOCK\n\nCan be created with makePSOCKCluster\nCreates brand new R Sessions (so nothing is inherited from the master), e.g.\n# This creates a cluster with 4 R sessions\ncl &lt;- makePSOCKCluster(4)\nChild sessions are connected to the master session via Socket connections\nCan be created outside the current computer, i.e., across multiple computers!"
  },
  {
    "objectID": "parallel_pkg.html#types-of-clusters-fork",
    "href": "parallel_pkg.html#types-of-clusters-fork",
    "title": "The parallel R package",
    "section": "Types of clusters: Fork",
    "text": "Types of clusters: Fork\n\nFork Cluster makeForkCluster:\nUses OS Forking,\nCopies the current R session locally (so everything is inherited from the master up to that point).\nData is only duplicated if altered (need to double check when this happens!)\nNot available on Windows.\n\nOther makeCluster: passed to snow (Simple Network of Workstations)"
  },
  {
    "objectID": "parallel_pkg.html#ex-1-parallel-rng-with-makepsockcluster",
    "href": "parallel_pkg.html#ex-1-parallel-rng-with-makepsockcluster",
    "title": "The parallel R package",
    "section": "Ex 1: Parallel RNG with makePSOCKCluster",
    "text": "Ex 1: Parallel RNG with makePSOCKCluster\n\n\n\n\n\n\nCaution\n\n\n\nUsing more threads than cores available on your computer is never a good idea. As a rule of thumb, clusters should be created using parallel::detectCores() - 1 cores (so you leave one free for the rest of your computer.)\n\n\n\n# 1. CREATING A CLUSTER\nlibrary(parallel)\nnnodes &lt;- 4L\ncl     &lt;- makePSOCKcluster(nnodes)    \n# 2. PREPARING THE CLUSTER\nclusterSetRNGStream(cl, 123) # Equivalent to `set.seed(123)`\n# 3. DO YOUR CALL\nans &lt;- parSapply(cl, 1:nnodes, function(x) runif(1e3))\n(ans0 &lt;- var(ans))\n\n              [,1]          [,2]          [,3]          [,4]\n[1,]  0.0861888293 -0.0001633431  5.939143e-04 -3.672845e-04\n[2,] -0.0001633431  0.0853841838  2.390790e-03 -1.462154e-04\n[3,]  0.0005939143  0.0023907904  8.114219e-02 -4.714618e-06\n[4,] -0.0003672845 -0.0001462154 -4.714618e-06  8.467722e-02\n\n\nMaking sure it is reproducible\n\n# I want to get the same!\nclusterSetRNGStream(cl, 123)\nans1 &lt;- var(parSapply(cl, 1:nnodes, function(x) runif(1e3)))\n# 4. STOP THE CLUSTER\nstopCluster(cl)\nall.equal(ans0, ans1) # All equal!\n\n[1] TRUE"
  },
  {
    "objectID": "parallel_pkg.html#ex-2-parallel-rng-with-makeforkcluster",
    "href": "parallel_pkg.html#ex-2-parallel-rng-with-makeforkcluster",
    "title": "The parallel R package",
    "section": "Ex 2: Parallel RNG with makeForkCluster",
    "text": "Ex 2: Parallel RNG with makeForkCluster\nIn the case of makeForkCluster\n\n# 1. CREATING A CLUSTER\nlibrary(parallel)\n# The fork cluster will copy the -nsims- object\nnsims  &lt;- 1e3\nnnodes &lt;- 4L\ncl     &lt;- makeForkCluster(nnodes)    \n# 2. PREPARING THE CLUSTER\nclusterSetRNGStream(cl, 123)\n# 3. DO YOUR CALL\nans &lt;- do.call(cbind, parLapply(cl, 1:nnodes, function(x) {\n  runif(nsims) # Look! we use the nsims object!\n               # This would have fail in makePSOCKCluster\n               # if we didn't copy -nsims- first.\n  }))\n(ans0 &lt;- var(ans))\n\n              [,1]          [,2]          [,3]          [,4]\n[1,]  0.0861888293 -0.0001633431  5.939143e-04 -3.672845e-04\n[2,] -0.0001633431  0.0853841838  2.390790e-03 -1.462154e-04\n[3,]  0.0005939143  0.0023907904  8.114219e-02 -4.714618e-06\n[4,] -0.0003672845 -0.0001462154 -4.714618e-06  8.467722e-02\n\n\nAgain, we want to make sure this is reproducible\n\n# Same sequence with same seed\nclusterSetRNGStream(cl, 123)\nans1 &lt;- var(do.call(cbind, parLapply(cl, 1:nnodes, function(x) runif(nsims))))\nans0 - ans1 # A matrix of zeros\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n[4,]    0    0    0    0\n\n# 4. STOP THE CLUSTER\nstopCluster(cl)\n\nWell, if you are a Mac-OS/Linux user, there’s a more straightforward way of doing this…"
  },
  {
    "objectID": "parallel_pkg.html#ex-3-parallel-rng-with-mclapply-forking-on-the-fly",
    "href": "parallel_pkg.html#ex-3-parallel-rng-with-mclapply-forking-on-the-fly",
    "title": "The parallel R package",
    "section": "Ex 3: Parallel RNG with mclapply (Forking on the fly)",
    "text": "Ex 3: Parallel RNG with mclapply (Forking on the fly)\nIn the case of mclapply, the forking (cluster creation) is done on the fly!\n\n# 1. CREATING A CLUSTER\nlibrary(parallel)\n# The fork cluster will copy the -nsims- object\nnsims  &lt;- 1e3\nnnodes &lt;- 4L\n# cl     &lt;- makeForkCluster(nnodes) # mclapply does it on the fly\n# 2. PREPARING THE CLUSTER\nset.seed(123) \n# 3. DO YOUR CALL\nans &lt;- do.call(cbind, mclapply(1:nnodes, function(x) runif(nsims)))\n(ans0 &lt;- var(ans))\n\n             [,1]        [,2]         [,3]         [,4]\n[1,]  0.085384184 0.002390790  0.006576204 -0.003998278\n[2,]  0.002390790 0.081142190  0.001846963  0.001476244\n[3,]  0.006576204 0.001846963  0.085175347 -0.002807348\n[4,] -0.003998278 0.001476244 -0.002807348  0.082425477\n\n\nOnce more, we want to make sure this is reproducible\n\n# Same sequence with same seed\nset.seed(123) \nans1 &lt;- var(do.call(cbind, mclapply(1:nnodes, function(x) runif(nsims))))\nans0 - ans1 # A matrix of zeros\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n[4,]    0    0    0    0\n\n# 4. STOP THE CLUSTER\n# stopCluster(cl) no need of doing this anymore"
  },
  {
    "objectID": "parallel_pkg.html#exercise-overhead-costs",
    "href": "parallel_pkg.html#exercise-overhead-costs",
    "title": "The parallel R package",
    "section": "Exercise: Overhead costs",
    "text": "Exercise: Overhead costs\nCompare the timing of taking the sum of 100 numbers when parallelized versus not. For the unparallized (serialized) version, use the following:\n\nset.seed(123)\nx &lt;- runif(n=100)\n\nserial_sum &lt;- function(x){\n  x_sum &lt;- sum(x)\n  return(x_sum)\n}\n\nFor the parallized version, follow this outline\n\nlibrary(parallel)\n\n\nset.seed(123)\nx &lt;- runif(n=100)\n\nparallel_sum &lt;- function(){\n  \n  \n  # Set number of cores to use\n  # make cluster and export to the cluster the x variable\n  # Use \"split function to divide x up into as many chunks as the number of cores\n  \n  # Calculate partial sums doing something like:\n  \n  partial_sums &lt;- parallel::parSapply(cl, x_split, sum)\n  \n  # Stop the cluster\n  \n  # Add and return the partial sums\n  \n}\n\nCompare the timing of the two approaches:\n\nmicrobenchmark::microbenchmark(\n  serial   = serial_sum(x),\n  parallel = parallel_sum(x),\n  times    = 10,\n  unit     = \"relative\"\n)"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "R code can be very efficient for typical tasks, but, as the code starts to increase in complexity, it is easy for it to become inefficient.\nSome quick R tips for efficient computing code:\n\nUse vectorized operations instead of loops.\nTry to use caching to avoid repeated calculations. Caching can also be done out of memory!\nAvoid unnecessary steps/data processing.\nReduce the number of copy operations.\nForeSITE"
  },
  {
    "objectID": "intro.html#vectorized-operations",
    "href": "intro.html#vectorized-operations",
    "title": "Introduction",
    "section": "Vectorized Operations",
    "text": "Vectorized Operations\nVectorization can mean many things in programming, but in R, vectorization refers to using functions over vectors. For instance, instead of using a loop to add two vectors together, you can use the + operator directly on the vectors:\n\n# Using a loop\nset.seed(331)\na &lt;- runif(1e3)\nb &lt;- runif(1e3)\nresult &lt;- numeric(length(a))\nfor (i in seq_along(a)) {\n  result[i] &lt;- a[i] + b[i]\n}\n\n# Using vectorized operation\nresult &lt;- a + b\n\nWe can even bechmark the performance of these two approaches:\n\nlibrary(microbenchmark)\nmicrobenchmark(\n  loop = {\n    result &lt;- numeric(length(a))\n    for (i in seq_along(a)) {\n      result[i] &lt;- a[i] + b[i]\n    }\n  },\n  vectorized = {\n    result &lt;- a + b\n  },\n  unit = \"relative\"\n)\n\nUnit: relative\n       expr    min       lq     mean  median       uq      max neval\n       loop 3608.2 1643.962 760.1619 948.375 636.9737 298.3105   100\n vectorized    1.0    1.000   1.0000   1.000   1.0000   1.0000   100\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor-loops are not always bad. The main issue is with the code inside of the for-loop. If the code is already vectorized, then there’s no need to remove the for-loop (unless you can vectorize the for-loop itself)."
  },
  {
    "objectID": "intro.html#caching-calculations",
    "href": "intro.html#caching-calculations",
    "title": "Introduction",
    "section": "Caching calculations",
    "text": "Caching calculations\nMany times, it is useful to cache calculations that are expensive to compute. For instance, if you have a function that takes a long time to run, you can store the result in a variable and reuse it later instead of recalculating it.\nHere is a bad example using the Fibonacci sequence:\n\nfibonacci &lt;- function(n) {\n  if (n &lt;= 1) {\n    return(n)\n  }\n  return(fibonacci(n - 1) + fibonacci(n - 2))\n}\n\nfibonacci_cached &lt;- function(n) {\n  prev &lt;- numeric(n + 1)\n  for (i in seq_len(n)) {\n    if (i &lt;= 1) {\n      prev[i + 1] &lt;- i\n    } else {\n      prev[i + 1] &lt;- prev[i] + prev[i - 1]\n    }\n  }\n\n  return(prev[n + 1])\n}\n\nBoth of these functions should return the same result, but the second is significantly faster as it avoids calling the function recursively:\n\nmicrobenchmark(\n  fibonacci(10),\n  fibonacci_cached(10),\n  times = 10,\n  unit = \"relative\",\n  check = \"equal\"\n)\n\nUnit: relative\n                 expr min       lq    mean   median       uq      max neval\n        fibonacci(10)  24 19.53846 13.2241 18.01724 16.13889 4.459459    10\n fibonacci_cached(10)   1  1.00000  1.0000  1.00000  1.00000 1.000000    10"
  },
  {
    "objectID": "intro.html#caching-calculations-bis",
    "href": "intro.html#caching-calculations-bis",
    "title": "Introduction",
    "section": "Caching calculations (bis)",
    "text": "Caching calculations (bis)\nIn the case of large calculations, we can also save results to the disk. For example, if we are running a simulation/computation, one per city/scenario, we can save the results to a file and read them later. Here is how to do it:\nFor each value of i, do the following:\n\nCheck if the file result_i.rds exists.\nIf it does not exist, run the computation and save the result to result_i.rds.\nIf it does exist, read the result from result_i.rds.\n\nAs simple as that! Here is an example using R code:\n\n# A complicated simulation function\nsimulate &lt;- function(i, seed) {\n  set.seed(seed)\n  rnorm(1e5)\n}\n\n# Generating seeds for each iteration\nset.seed(331)\nnsims &lt;- 100\nseeds &lt;- sample.int(.Machine$integer.max, nsims)\n\n# Just for this example, we will use a tempfile\nres_0 &lt;- vector(\"list\", length = nsims)\nfor (i in seq_len(nsims)) {\n  \n  # Creating the filename\n  fn &lt;- file.path(tempdir(), paste0(i, \".rds\"))\n\n  # Does the result already exist?\n  if (file.exists(fn))\n    res_0[[i]] &lt;- readRDS(fn)\n  else {\n    # If not, run the simulation and save the result\n    res_0[[i]] &lt;- simulate(i, seed = i)\n    saveRDS(res_0[[i]], fn)\n  }\n\n}\n\n\n\n\n\n\n\nTip\n\n\n\nWhen running simulations, it is a good pracitice to set individual seeds for each simulation (if these are individually complex). That way, if the code fails, you can rerun only the failed simulations without having to redo all of them.\n\n\nFurthermore, it is a good idea to wrap your code in a tryCatch() call to handle errors gracefully. This way, if a simulation fails, you can log the error and continue with the next simulation without stopping the entire process.\n\n# Just for this example, we will use a tempfile\nres_0 &lt;- vector(\"list\", length = nsims)\nfor (i in seq_len(nsims)) {\n  \n  # Creating the filename\n  fn &lt;- file.path(tempdir(), paste0(i, \".rds\"))\n\n  # Does the result already exist?\n  res &lt;- tryCatch({\n    if (file.exists(fn))\n      readRDS(fn)\n    else {\n      # If not, run the simulation and save the result\n      ans_i &lt;- simulate(i, seed = i)\n      saveRDS(ans_i, fn)\n      ans_i\n    }\n  }, error = function(e) e)\n\n  if (inherits(res, \"error\")) {\n    message(\"Simulation \", i, \" failed: \", res$message)\n    next  # Skip to the next iteration\n  }\n\n  # We still store it, even if it failed\n  res_0[[i]] &lt;- res\n\n}"
  },
  {
    "objectID": "intro.html#avoiding-unnecessary-steps",
    "href": "intro.html#avoiding-unnecessary-steps",
    "title": "Introduction",
    "section": "Avoiding unnecessary steps",
    "text": "Avoiding unnecessary steps\nMany times, we can find shortcuts to reduce the amount of data processing we need to do. A great example is in the linear regression function lm(). The lm() function will go beyond finding the coefficients in a linear model, it will also compute residuals, fitted values, and more. Instead, we can use the function lm.fit() which only computes the coefficients:\n\nset.seed(331)\nx &lt;- rnorm(2e3)\ny &lt;- 2 + 3 * x + rnorm(2e3)\n\n# Comparing\nmicrobenchmark(\n  lm = coef(lm(y ~ x)),\n  lm_fit = coef(lm.fit(cbind(1, x), y)),\n  times = 10,\n  unit = \"relative\"\n)\n\nUnit: relative\n   expr     min       lq     mean  median       uq      max neval\n     lm 3.92569 3.995119 6.458811 3.92213 3.833866 23.58231    10\n lm_fit 1.00000 1.000000 1.000000 1.00000 1.000000  1.00000    10"
  },
  {
    "objectID": "intro.html#reducing-copy-operations",
    "href": "intro.html#reducing-copy-operations",
    "title": "Introduction",
    "section": "Reducing copy operations",
    "text": "Reducing copy operations\nLike in any programming language, copy operations in R can be expensive. Beyond increasing the amount of memory used, copy operations require tme to allocate memory and then copy the data. Modern R minimizes these by using copy-on-modify. This means that R will not copy an object until it is modified. For example, the following code makes multiple copies of X, but it is until the last line that R actually makes a copy of X:\n\nset.seed(331)\nX &lt;- runif(1e4)\nY &lt;- X\nZ &lt;- X\n\n# Checking the address of the objects\nlibrary(lobstr)\nobj_addr(X)\n## [1] \"0x141970000\"\nobj_addr(Y)\n## [1] \"0x141970000\"\nobj_addr(Z)\n## [1] \"0x141970000\"\n\nModifying X will trigger a copy operation, and the addresses of Y and Z will remain the same, while X will have a new address:\n\n# Modifying X\nX[1] &lt;- 100  # This is when R makes a copy of X\nobj_addr(X)\n## [1] \"0x1305f8000\"\nobj_addr(Y)\n## [1] \"0x141970000\"\nobj_addr(Z)\n## [1] \"0x141970000\""
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "Program",
    "section": "",
    "text": "This half-day workshop will focus on practical techniques for writing efficient R code. Participants will learn about vectorization, parallel computing, and specialized packages to dramatically improve their R code performance. The session will include hands-on exercises and real-world examples.\nTBD\nForeSITE"
  },
  {
    "objectID": "program.html#resources-and-git-materials",
    "href": "program.html#resources-and-git-materials",
    "title": "Program",
    "section": "Resources and Git Materials",
    "text": "Resources and Git Materials\nThe workshop will utilize materials from the git folder for version control best practices when working with performance-critical code. Additional resources and example datasets will be provided during the workshop."
  },
  {
    "objectID": "participants.html",
    "href": "participants.html",
    "title": "Participants",
    "section": "",
    "text": "Some of the participants of the workshop are listed below:\n\n\nGeorge G Vega Yon (pronouns: He/Him/Él) University of Utah   \n\nI am an Assistant Professor of Research at the Division of Epidemiology at the University of Utah. I work on studying Complex Systems using Statistical Computing. I have over ten years of experience developing scientific software focusing on high-performance computing, data visualization, and social network analysis. My training is in Public Policy (M.A. UAI, 2011), Economics (M.Sc. Caltech, 2015), and Biostatistics (Ph.D. USC, 2020).\n\n\n\n\nForeSITE"
  },
  {
    "objectID": "profiling.html",
    "href": "profiling.html",
    "title": "Profiling",
    "section": "",
    "text": "Code profiling is a fundamental tool for programmers. With profiling, it is possible to identify potential bottle necks in your code, as well as exessive memory usage. Some important things to consider when profiling your code:\nForeSITE"
  },
  {
    "objectID": "profiling.html#profiling-code-in-r",
    "href": "profiling.html#profiling-code-in-r",
    "title": "Profiling",
    "section": "Profiling code in R",
    "text": "Profiling code in R\nIn the R programming language, the most used profiling tool comes with the profvis package. The package provides a wrapper of the Rprof function, which is a built-in R function for profiling code. The profvis package makes it easier to visualize the profiling results in a web-based interface.\nTo use profvis, you first need to install it from CRAN:\ninstall.packages(\"profvis\")\nThen, you can use it to profile your R code like this:\nlibrary(profvis)\n\nprofvis({\n  # Your R code here\n})\nThis will execute the code and generate a visualization of the profiling results in a new browser window. You can also save the output using the htmlwidgets package:\npv &lt;- profvis({\n  # Your R code here\n})\n\nhtmlwidgets::saveWidget(pv, \"profvis.html\")\nOnce open, you will see two visualizations: the flamegraph and the data. The flamegraph is one of the most useful visualizations. It directly maps the time and memory used by each line of code\n\nThe data visualization shows the distribution of time spent in each function. Using a tree structure, which allows taking deep dives into the call stack."
  },
  {
    "objectID": "profiling.html#exercise-identifying-the-bottle-neck",
    "href": "profiling.html#exercise-identifying-the-bottle-neck",
    "title": "Profiling",
    "section": "Exercise: Identifying the bottle neck",
    "text": "Exercise: Identifying the bottle neck\n\n# Generate data\ntimes &lt;- 4e5\ncols &lt;- 150\ndata &lt;- as.data.frame(x = matrix(rnorm(times * cols, mean = 5), ncol = cols))\ndata &lt;- cbind(id = paste0(\"g\", seq_len(times)), data)\n\npv &lt;- profvis::profvis({\n  data1 &lt;- data   # Store in another variable for this run\n\n  # Get column means\n  means &lt;- apply(data1[, names(data1) != \"id\"], 2, mean)\n\n  # Subtract mean from each column\n  for (i in seq_along(means)) {\n    data1[, names(data1) != \"id\"][, i] &lt;- data1[, names(data1) != \"id\"][, i] - means[i]\n  }\n})\n\nhtmlwidgets::saveWidget(pv, \"profvis-slow-code.html\")\n\n# In interactive mode, we can directly view the profiling results\nif (interactive())\n  print(pv)\n\n\n\nCan you identify where is the bottleneck in the code? What would you do to speed it up?"
  }
]