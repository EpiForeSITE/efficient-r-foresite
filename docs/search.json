[
  {
    "objectID": "data.table.html",
    "href": "data.table.html",
    "title": "The data.table R package",
    "section": "",
    "text": "For most cases, the most proper data wrangling tool in R is the dplyr package. Nonetheless, when dealing with large amounts of data, the data.table package is the fastest alternative available for doing data processing within R (see the benchmarks).\nForeSITE"
  },
  {
    "objectID": "data.table.html#reading-and-writing-data",
    "href": "data.table.html#reading-and-writing-data",
    "title": "The data.table R package",
    "section": "Reading and Writing Data",
    "text": "Reading and Writing Data\nReading and writing operations with data.table’s fread and fwrite are highly optimized. Here is a benchmark we can do on our own:\n\nlibrary(readr) # tidyverse's\nlibrary(data.table)\nlibrary(microbenchmark)\n\n# Generating a large dataset of random numbers\nset.seed(1231)\nx &lt;- runif(5e4 * 10) |&gt; matrix(ncol = 10) |&gt; data.frame()\n\n# Creating tempfiles\ntemp_dt &lt;- tempfile(fileext = \".csv\")\ntemp_tv &lt;- tempfile(fileext = \".csv\")\ntemp_r  &lt;- tempfile(fileext = \".csv\")\n\n\nbm &lt;- microbenchmark(\n  readr      = write_csv(x, temp_tv, num_threads = 1L, progress = FALSE),\n  data.table = fwrite(\n    x, temp_dt, verbose = FALSE, nThread = 1L,\n    showProgress = FALSE),\n  base       = write.csv(x, temp_r),\n  times = 20\n)\n\nWarning in microbenchmark(readr = write_csv(x, temp_tv, num_threads = 1L, :\nless accurate nanosecond times to avoid potential integer overflows\n\nbm\n\nUnit: milliseconds\n       expr       min        lq      mean    median        uq      max neval\n      readr  20.50656  20.85293  32.74184  22.92738  28.40509 192.2000    20\n data.table  21.73935  23.39716  25.49071  24.40361  25.86483  32.4754    20\n       base 219.56336 230.62471 239.80889 240.37378 247.95769 260.8928    20\n\n# We can also visualize it\nbm |&gt;\n  plot(\n    log = \"y\",\n    ylab = \"Time (ms) (log10-scale)\",\n    main = \"CSV Writing Benchmark\"\n  )\n\n\n\n\n\n\n\n\nThe same thing applies when reading data\n\n# Writing the data\nfwrite(x, temp_r, verbose = FALSE, nThread = 1L, showProgress = FALSE)\n\n# Benchmarking\nbm &lt;- microbenchmark(\n  readr      = read_csv(\n    temp_r, progress = FALSE, num_threads = 1L,\n    show_col_types = FALSE\n    ),\n  data.table = fread(\n    temp_r, verbose = FALSE, nThread = 1L,\n    showProgress = FALSE\n    ),\n  base       = read.csv(temp_r),\n  times = 10\n)\n\nWarning in microbenchmark(readr = read_csv(temp_r, progress = FALSE,\nnum_threads = 1L, : less accurate nanosecond times to avoid potential integer\noverflows\n\nbm\n\nUnit: milliseconds\n       expr        min        lq      mean    median        uq       max neval\n      readr  40.088898  40.28516  74.29501  41.79661  42.52569 370.04095    10\n data.table   9.947297  10.24852  11.92535  10.63159  11.27336  21.29651    10\n       base 148.565591 176.56457 204.73412 199.02650 235.19945 247.29724    10\n\nbm |&gt;\n  plot(\n    log = \"y\",\n    ylab = \"Time (ms) (log10-scale)\",\n    main = \"CSV Reading Benchmark\"\n  )\n\n\n\n\n\n\n\n\nUnder the hood, the readr package uses the vroom package. Nonetheless, there are some operations (dealing with character data mostly), where the vroom package shines. Regardless, data.table is a perfect alternative as it goes beyond just reading/writing data."
  },
  {
    "objectID": "data.table.html#example-data-manipulation",
    "href": "data.table.html#example-data-manipulation",
    "title": "The data.table R package",
    "section": "Example data manipulation",
    "text": "Example data manipulation\ndata.table is also the fastest for data manipulation. Here are a couple of examples aggregating data with data table vs dplyr\n\ninput &lt;- if (file.exists(\"flights14.csv\")) {\n   \"flights14.csv\"\n} else {\n  \"https://raw.githubusercontent.com/Rdatatable/data.table/master/vignettes/flights14.csv\"\n}\n\n# Reading the data\nflights_dt &lt;- fread(input)\nflights_tb &lt;- read_csv(input, show_col_types = FALSE)\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:data.table':\n\n    between, first, last\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# To avoid some messaging from the function\noptions(dplyr.summarise.inform = FALSE)\n\nmicrobenchmark(\n  data.table = flights_dt[, .N, by = .(origin, dest)],\n  dplyr = flights_tb |&gt;\n    group_by(origin, dest) |&gt;\n    summarise(n = n())\n)\n\nUnit: milliseconds\n       expr      min       lq     mean   median       uq       max neval\n data.table 1.788051 2.121955 2.789121 2.367279 2.963931  5.913922   100\n      dplyr 5.255011 5.544102 6.682056 5.832742 6.371748 37.637713   100"
  },
  {
    "objectID": "parallel.html",
    "href": "parallel.html",
    "title": "Parallel computing with R",
    "section": "",
    "text": "Note\n\n\n\nThis content was originally published in the book “Applied HPC with R” by George G. Vega Yon, Ph.D. You can find the book at https://book-hpc.ggvy.cl\nWhile most people see R as a slow programming language, it has powerful features that dramatically accelerate your code 1. Although R wasn’t necessarily built for speed, there are some tools and ways in which we can accelerate R. This chapter introduces what we will understand as High-performance computing in R.\nForeSITE"
  },
  {
    "objectID": "parallel.html#high-performance-computing-an-overview",
    "href": "parallel.html#high-performance-computing-an-overview",
    "title": "Parallel computing with R",
    "section": "High-Performance Computing: An overview",
    "text": "High-Performance Computing: An overview\nFrom R’s perspective, we can think of HPC in terms of two or three things:2 Big data, parallel computing, and compiled code.\n\nBig Data\nWhen we talk about big data, we refer to cases where your computer struggles to handle a dataset. A typical example of the latter is when the number of observations (rows) in your data frame is too many to fit a linear regression model. Instead of buying a bigger computer, there are many good solutions to solve memory-related problems:\n\nOut-of-memory storage. The idea is simple, instead of using your RAM to load the data, use other methods to load the data. Two notewirthy alternatives are the bigmemory and implyr R packages. The bigmemory package provides methods for using “file-backed” matrices. On the other hand, implyr implements a wrapper to access Apache Impala, an SQL query engine for a cluster running Apache Hadoop.\nEfficient algorithms for big data: To avoid running out of memory with your regression analysis, the R packages biglm and biglasso deliver highly-efficient alternatives to glm and glmnet, respectively. Now, if your data fits your RAM, but you still struggle with data wrangling, the data.table package is the solution.\nStore it more efficiently: Finally, when it comes to linear algebra, the Matrix R package shines with its formal classes and methods for managing Sparse Matrices, i.e., big matrices whose entries are primarily zeros; for example, the dgCMatrix objects. Furthermore, Matrix comes shipped with R, which makes it even more appealing.\n\n\n\nParallel computing\n\n\n\n\n\nFlynn’s Classical Taxonomy (Blaise Barney, Introduction to Parallel Computing, Lawrence Livermore National Laboratory)\n\n\n\n\nWe will focus on the Single Instruction stream Multiple Data stream.\nIn general terms, a parallel computing program is one in which we use two or more computational threads simultaneously. Although computational thread usually means core, there are multiple levels at which a computer program can be parallelized. To understand this, we first need to see what composes a modern computer:\n\n\n\nSource: Original figure from LUMI consortium documentation [@lumi2023]\n\n\nStreaming SIMD Extensions [SSE] and Advanced Vector Extensions [AVX]\n\nSerial vs. Parallel\n\n\n\n\n\nSource: Blaise Barney, Introduction to Parallel Computing, Lawrence Livermore National Laboratory\n\n\n\n\nsource: Blaise Barney, Introduction to Parallel Computing, Lawrence Livermore National Laboratory\n\n\n\n\n\nHigh-performance computing in R\n\nSome vocabulary for HPC\nIn raw terms\n\nSupercomputer: A single big machine with thousands of cores/GPGPUs.\nHigh-Performance Computing (HPC): Multiple machines within a single network.\nHigh Throughput Computing (HTC): Multiple machines across multiple networks.\n\nYou may not have access to a supercomputer, but certainly, HPC/HTC clusters are more accessible these days, e.g., AWS provides a service to create HPC clusters at a low cost (allegedly, since nobody understands how pricing works)\n\n\n\nGPU vs. CPU\n\n\n\n\n\nNVIDIA Blog\n\n\n\n\n\nWhy use OpenMP if GPU is suited to compute-intensive operations? Well, mostly because OpenMP is VERY easy to implement (easier than CUDA, which is the easiest way to use GPU).3\n\n\n\nWhen is it a good idea?\n\n\n\n\n\nAsk yourself these questions before jumping into HPC!\n\n\n\n\n\n\nParallel computing in R\nWhile there are several alternatives (just take a look at the High-Performance Computing Task View), we’ll focus on the following R-packages for explicit parallelism:\n\nparallel: R package that provides ‘[s]upport for parallel computation, including random-number generation’.\nfuture: ‘[A] lightweight and unified Future API for sequential and parallel processing of R expression via futures.’\nRcpp + OpenMP: Rcpp is an R package for integrating R with C++ and OpenMP is a library for high-level parallelism for C/C++ and FORTRAN.\n\nOthers but not used here\n\nforeach for iterating through lists in parallel.\nRmpi for creating MPI clusters.\n\nAnd tools for implicit parallelism (out-of-the-box tools that allow the programmer not to worry about parallelization):\n\ngpuR for Matrix manipulation using GPU\ntensorflow an R interface to TensorFlow.\n\nA ton of other types of resources, notably the tools for working with batch schedulers such as Slurm, and HTCondor."
  },
  {
    "objectID": "parallel.html#footnotes",
    "href": "parallel.html#footnotes",
    "title": "Parallel computing with R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNonetheless, this claim can be said about almost any programming language; there are notable examples like the R package data.table [@datatable] which has been demonstrated to out-perform most data wrangling tools.↩︎\nMake sure to check out CRAN Task View on HPC.↩︎\nSadia National Laboratories started the Kokkos project, which provides a one-fits-all C++ library for parallel programming. More information on the Kokkos’s wiki site.↩︎"
  },
  {
    "objectID": "efficiency.html",
    "href": "efficiency.html",
    "title": "Writing efficient code",
    "section": "",
    "text": "R code can be very efficient for typical tasks, but, as the code starts to increase in complexity, it is easy for it to become inefficient.\nSome quick R tips for efficient computing code:\n\nUse vectorized operations instead of loops.\nTry to use caching to avoid repeated calculations. Caching can also be done out of memory!\nAvoid unnecessary steps/data processing.\nReduce the number of copy operations.\nForeSITE"
  },
  {
    "objectID": "efficiency.html#vectorized-operations",
    "href": "efficiency.html#vectorized-operations",
    "title": "Writing efficient code",
    "section": "Vectorized Operations",
    "text": "Vectorized Operations\nVectorization can mean many things in programming, but in R, vectorization refers to using functions over vectors. For instance, instead of using a loop to add two vectors together, you can use the + operator directly on the vectors:\n\n# Using a loop\nset.seed(331)\na &lt;- runif(1e3)\nb &lt;- runif(1e3)\nresult &lt;- numeric(length(a))\nfor (i in seq_along(a)) {\n  result[i] &lt;- a[i] + b[i]\n}\n\n# Using vectorized operation\nresult &lt;- a + b\n\nWe can even bechmark the performance of these two approaches:\n\nlibrary(microbenchmark)\nmicrobenchmark(\n  loop = {\n    result &lt;- numeric(length(a))\n    for (i in seq_along(a)) {\n      result[i] &lt;- a[i] + b[i]\n    }\n  },\n  vectorized = {\n    result &lt;- a + b\n  },\n  unit = \"relative\"\n)\n\nUnit: relative\n       expr    min      lq     mean median       uq      max neval\n       loop 3550.6 1932.25 1248.392   1544 1060.538 755.9273   100\n vectorized    1.0    1.00    1.000      1    1.000   1.0000   100\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor-loops are not always bad. The main issue is with the code inside of the for-loop. If the code is already vectorized, then there’s no need to remove the for-loop (unless you can vectorize the for-loop itself)."
  },
  {
    "objectID": "efficiency.html#caching-calculations",
    "href": "efficiency.html#caching-calculations",
    "title": "Writing efficient code",
    "section": "Caching calculations",
    "text": "Caching calculations\nMany times, it is useful to cache calculations that are expensive to compute. For instance, if you have a function that takes a long time to run, you can store the result in a variable and reuse it later instead of recalculating it.\nHere is a bad example using the Fibonacci sequence:\n\nfibonacci &lt;- function(n) {\n  if (n &lt;= 1) {\n    return(n)\n  }\n  return(fibonacci(n - 1) + fibonacci(n - 2))\n}\n\nfibonacci_cached &lt;- function(n) {\n  prev &lt;- numeric(n + 1)\n  for (i in seq_len(n)) {\n    if (i &lt;= 1) {\n      prev[i + 1] &lt;- i\n    } else {\n      prev[i + 1] &lt;- prev[i] + prev[i - 1]\n    }\n  }\n\n  return(prev[n + 1])\n}\n\nBoth of these functions should return the same result, but the second is significantly faster as it avoids calling the function recursively:\n\nmicrobenchmark(\n  fibonacci(10),\n  fibonacci_cached(10),\n  times = 10,\n  unit = \"relative\",\n  check = \"equal\"\n)\n\nUnit: relative\n                 expr      min       lq    mean   median    uq      max neval\n        fibonacci(10) 26.21053 23.90476 16.7649 23.02273 20.44 5.298969    10\n fibonacci_cached(10)  1.00000  1.00000  1.0000  1.00000  1.00 1.000000    10"
  },
  {
    "objectID": "efficiency.html#caching-calculations-bis",
    "href": "efficiency.html#caching-calculations-bis",
    "title": "Writing efficient code",
    "section": "Caching calculations (bis)",
    "text": "Caching calculations (bis)\nIn the case of large calculations, we can also save results to the disk. For example, if we are running a simulation/computation, one per city/scenario, we can save the results to a file and read them later. Here is how to do it:\nFor each value of i, do the following:\n\nCheck if the file result_i.rds exists.\nIf it does not exist, run the computation and save the result to result_i.rds.\nIf it does exist, read the result from result_i.rds.\n\nAs simple as that! Here is an example using R code:\n\n# A complicated simulation function\nsimulate &lt;- function(i, seed) {\n  set.seed(seed)\n  rnorm(1e5)\n}\n\n# Generating seeds for each iteration\nset.seed(331)\nnsims &lt;- 100\nseeds &lt;- sample.int(.Machine$integer.max, nsims)\n\n# Just for this example, we will use a tempfile\nres_0 &lt;- vector(\"list\", length = nsims)\nfor (i in seq_len(nsims)) {\n  \n  # Creating the filename\n  fn &lt;- file.path(tempdir(), paste0(i, \".rds\"))\n\n  # Does the result already exist?\n  if (file.exists(fn))\n    res_0[[i]] &lt;- readRDS(fn)\n  else {\n    # If not, run the simulation and save the result\n    res_0[[i]] &lt;- simulate(i, seed = i)\n    saveRDS(res_0[[i]], fn)\n  }\n\n}\n\n\n\n\n\n\n\nTip\n\n\n\nWhen running simulations, it is a good pracitice to set individual seeds for each simulation (if these are individually complex). That way, if the code fails, you can rerun only the failed simulations without having to redo all of them.\n\n\nFurthermore, it is a good idea to wrap your code in a tryCatch() call to handle errors gracefully. This way, if a simulation fails, you can log the error and continue with the next simulation without stopping the entire process.\n\n# Just for this example, we will use a tempfile\nres_0 &lt;- vector(\"list\", length = nsims)\nfor (i in seq_len(nsims)) {\n  \n  # Creating the filename\n  fn &lt;- file.path(tempdir(), paste0(i, \".rds\"))\n\n  # Does the result already exist?\n  res &lt;- tryCatch({\n    if (file.exists(fn))\n      readRDS(fn)\n    else {\n      # If not, run the simulation and save the result\n      ans_i &lt;- simulate(i, seed = i)\n      saveRDS(ans_i, fn)\n      ans_i\n    }\n  }, error = function(e) e)\n\n  if (inherits(res, \"error\")) {\n    message(\"Simulation \", i, \" failed: \", res$message)\n    next  # Skip to the next iteration\n  }\n\n  # We still store it, even if it failed\n  res_0[[i]] &lt;- res\n\n}\n\n\n\n\n\n\n\nTip\n\n\n\nThe saveRDS function in R uses the compress = TRUE argument as default. Compressing the data for saving space is generally a good idea, but not if you need to read data fast. So, if space is not a constraint, you can set compress = FALSE when saving the RDS file to accelerate the reading process."
  },
  {
    "objectID": "efficiency.html#caching-calculations-in-a-shinyapp",
    "href": "efficiency.html#caching-calculations-in-a-shinyapp",
    "title": "Writing efficient code",
    "section": "Caching calculations in a ShinyApp",
    "text": "Caching calculations in a ShinyApp\nBelow is an example of a plotly figure that is pre-recorded for a shiny app. The idea is that, if the figure does not need to be reactive, you can always pre-compute the results and store them on a file, in this case, as an HTML file:\nlibrary(shiny)\nlibrary(bslib)\nlibrary(plotly)\n \n# Like we did with the simulations, we have a default filename\nfn &lt;- \"plotly.html\"\n \n# Notice I'm adding the www because, outside of the\n# server call, this writes directly to the top level.\n# Once reading, it will read from www.\nif (!file.exists(file.path(\"www\", fn))) {\n \n  message(\"Creating the file...\")\n \n  # if it doesn't exist, then it creates it and saves it\n  p &lt;- plot_ly(x = 1:10, y = 1:10) %&gt;% add_lines()\n  htmlwidgets::saveWidget(\n    p,\n    file = \"www/plotly.html\",\n    selfcontained = TRUE\n    )\n} else {\n  message(\"The file already exists!\")\n}\n \n \n# Define UI for app that draws a histogram ----\nui &lt;- page_sidebar(\n  # App title ----\n  title = \"Hello Shiny!\",\n  # Sidebar panel for inputs ----\n  sidebar = sidebar(\n    # Input: Slider for the number of bins ----\n    sliderInput(\n      inputId = \"bins\",\n      label = \"Number of bins:\",\n      min = 1,\n      max = 50,\n      value = 30\n    )\n  ),\n  htmlOutput(outputId = \"plotlyOutput\")\n)\n \nserver &lt;- function(input, output) {\n \n  output$plotlyOutput &lt;- renderUI({\n    tags$iframe(\n      src = \"plotly.html\"\n    )\n  })\n \n}\n \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "efficiency.html#avoiding-unnecessary-steps",
    "href": "efficiency.html#avoiding-unnecessary-steps",
    "title": "Writing efficient code",
    "section": "Avoiding unnecessary steps",
    "text": "Avoiding unnecessary steps\nMany times, we can find shortcuts to reduce the amount of data processing we need to do. A great example is in the linear regression function lm(). The lm() function will go beyond finding the coefficients in a linear model, it will also compute residuals, fitted values, and more. Instead, we can use the function lm.fit() which only computes the coefficients:\n\nset.seed(331)\nx &lt;- rnorm(2e3)\ny &lt;- 2 + 3 * x + rnorm(2e3)\n\n# Comparing\nmicrobenchmark(\n  lm = coef(lm(y ~ x)),\n  lm_fit = coef(lm.fit(cbind(1, x), y)),\n  times = 10,\n  unit = \"relative\"\n)\n\nUnit: relative\n   expr      min      lq     mean   median       uq      max neval\n     lm 4.122398 4.19202 6.967489 4.196407 4.360512 26.53458    10\n lm_fit 1.000000 1.00000 1.000000 1.000000 1.000000  1.00000    10"
  },
  {
    "objectID": "efficiency.html#reducing-copy-operations",
    "href": "efficiency.html#reducing-copy-operations",
    "title": "Writing efficient code",
    "section": "Reducing copy operations",
    "text": "Reducing copy operations\nLike in any programming language, copy operations in R can be expensive. Beyond increasing the amount of memory used, copy operations require tme to allocate memory and then copy the data. Modern R minimizes these by using copy-on-modify. This means that R will not copy an object until it is modified. For example, the following code makes multiple copies of X, but it is until the last line that R actually makes a copy of X:\n\nset.seed(331)\nX &lt;- runif(1e4)\nY &lt;- X\nZ &lt;- X\n\n# Checking the address of the objects\nlibrary(lobstr)\nobj_addr(X)\n## [1] \"0x129918000\"\nobj_addr(Y)\n## [1] \"0x129918000\"\nobj_addr(Z)\n## [1] \"0x129918000\"\n\nModifying X will trigger a copy operation, and the addresses of Y and Z will remain the same, while X will have a new address:\n\n# Modifying X\nX[1] &lt;- 100  # This is when R makes a copy of X\nobj_addr(X)\n## [1] \"0x158420000\"\nobj_addr(Y)\n## [1] \"0x129918000\"\nobj_addr(Z)\n## [1] \"0x129918000\""
  },
  {
    "objectID": "profiling.html",
    "href": "profiling.html",
    "title": "Profiling",
    "section": "",
    "text": "Code profiling is a fundamental tool for programmers. With profiling, it is possible to identify potential bottle necks in your code, as well as exessive memory usage. Some important things to consider when profiling your code:\nHere is a proposed workflow for profiling and optimizing your code in general\nForeSITE"
  },
  {
    "objectID": "profiling.html#a-proposed-workflow",
    "href": "profiling.html#a-proposed-workflow",
    "title": "Profiling",
    "section": "A proposed workflow",
    "text": "A proposed workflow\nHere is a formula to follow when doing code profiling/optimization:\n\nAsk yourself these questions before you jump into profiling:\n\n\n\n\n\n\nflowchart LR\n  A((Start))--&gt;B{Is it&lt;br&gt;slow?}\n  B--&gt;|Yes|C{Are you&lt;br&gt;sure?}\n  B--&gt;|Not really, it&lt;br&gt;just sounds cool...|D((End))\n  C--&gt;|Maybe...|E{Takes more&lt;br&gt;than 1min?}\n\n  C--&gt;|Yes|F{Is it&lt;br&gt;worth it?}\n  E--&gt;|Yes|F\n  E--&gt;|No|D\n\n  F--&gt;|No, I'm&lt;br&gt;just bored&lt;br&gt;|D\n  F--&gt;|Yes, my life&lt;br&gt;depends on it|G((Proceed with&lt;br&gt;optimization))\n\n\n\n\n\n\n\nIf you succeed, then ensure that the profiling is done in a finite time, this is, use a subset of the data to avoid having long waits. Running the profiler will add overhead computing time, so try to keep it short (e.g., 1 minute).\nThere may be many things that could be optimized, focus on what would deliver the highest impact. Could be a function that is only called once but takes a long time to execute, or a function that is called multiple times but is relatively fast.\nBefore making any changes, ensure that you have a backup of your original code, as well as a copy of the current profiling results. You should also ensure to save (if possible) the outcome of the code.\nRe-run the profiler and compare performance. If no changes are observed, then go back to step 2. Ensure the new version of the code maintains the same functionality as the original (check the results)."
  },
  {
    "objectID": "profiling.html#profiling-code-in-r",
    "href": "profiling.html#profiling-code-in-r",
    "title": "Profiling",
    "section": "Profiling code in R",
    "text": "Profiling code in R\nIn the R programming language, the most used profiling tool comes with the profvis package. The package provides a wrapper of the Rprof function, which is a built-in R function for profiling code. The profvis package makes it easier to visualize the profiling results in a web-based interface.\nTo use profvis, you first need to install it from CRAN:\ninstall.packages(\"profvis\")\nThen, you can use it to profile your R code like this:\nlibrary(profvis)\n\nprofvis({\n  # Your R code here\n})\nThis will execute the code and generate a visualization of the profiling results in a new browser window. You can also save the output using the htmlwidgets package:\npv &lt;- profvis({\n  # Your R code here\n})\n\nhtmlwidgets::saveWidget(pv, \"profvis.html\")\nOnce open, you will see two visualizations: the flamegraph and the data. The flamegraph is one of the most useful visualizations. It directly maps the time and memory used by each line of code\n\nThe data visualization shows the distribution of time spent in each function. Using a tree structure, which allows taking deep dives into the call stack.\n\n\n\n\n\n\n\nTip\n\n\n\nWhen developing R packages, it is a good idea to pair your profvis::profvis call with devtools::load_all() to ensure that all source code is available to the profiler. Otherwise, the flamegraph won’t show your code (it will say “unavailable”)."
  },
  {
    "objectID": "profiling.html#exercise-identifying-the-bottle-neckbottleneck",
    "href": "profiling.html#exercise-identifying-the-bottle-neckbottleneck",
    "title": "Profiling",
    "section": "Exercise: Identifying the bottle neck1",
    "text": "Exercise: Identifying the bottle neck1\n\n# Generate data\ntimes &lt;- 4e5\ncols &lt;- 150\ndata &lt;- as.data.frame(x = matrix(rnorm(times * cols, mean = 5), ncol = cols))\ndata &lt;- cbind(id = paste0(\"g\", seq_len(times)), data)\n\npv &lt;- profvis::profvis({\n  data1 &lt;- data   # Store in another variable for this run\n\n  # Get column means\n  means &lt;- apply(data1[, names(data1) != \"id\"], 2, mean)\n\n  # Subtract mean from each column\n  for (i in seq_along(means)) {\n    data1[, names(data1) != \"id\"][, i] &lt;- data1[, names(data1) != \"id\"][, i] - means[i]\n  }\n})\n\nhtmlwidgets::saveWidget(pv, \"profvis-slow-code.html\")\n\n# In interactive mode, we can directly view the profiling results\nif (interactive())\n  print(pv)\n\n\n\nCan you identify where is the bottleneck in the code? What would you do to speed it up?"
  },
  {
    "objectID": "profiling.html#footnotes",
    "href": "profiling.html#footnotes",
    "title": "Profiling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCode copied verbatim from the profvis R package here.↩︎"
  },
  {
    "objectID": "parallel_pkg.html",
    "href": "parallel_pkg.html",
    "title": "The parallel R package",
    "section": "",
    "text": "Note\n\n\n\nThis content was originally published in the book “Applied HPC with R” by George G. Vega Yon, Ph.D. You can find the book at https://book-hpc.ggvy.cl\nAlthough R was not built for parallel computing, multiple ways of parallelizing your R code exist. One of these is the parallel package. This R package, shipped with base R, provides various functions to parallelize R code using embarrassingly parallel computing, i.e., a divide-and-conquer-type strategy. The basic idea is to start multiple R sessions (usually called child processes), connect the main session with those, and send them instructions. This section goes over a common workflow to work with R’s parallel.\nForeSITE"
  },
  {
    "objectID": "parallel_pkg.html#parallel-workflow",
    "href": "parallel_pkg.html#parallel-workflow",
    "title": "The parallel R package",
    "section": "Parallel workflow",
    "text": "Parallel workflow\n\n\n\n\n\nflowchart LR\n  start((Start)) --&gt; one[\"`Create&lt;br&gt;a cluster`\"]\n  one --&gt; two\n  subgraph two[Prepare the session]\n    direction TB\n    copy[Copy objects]~~~eval[Evaluate&lt;br&gt;expressions]\n    eval~~~seed[Set seed]\n  end\n\n  two --&gt; three[Do your&lt;br&gt;call]\n  three --&gt; four[Stop the&lt;br&gt;cluster]\n\n\n\n\n\n\n(Usually) We do the following:\n\nCreate a PSOCK/FORK (or other) cluster using makePSOCKCluster/makeForkCluster (or makeCluster). How many child processes will depend on how many threads your computer has. A rule of thumb is to use parallel::detectCores() - 1 cores (so you leave one free for the rest of your computer).\nCopy/prepare each R session (if you are using a PSOCK cluster):\n\nCopy objects with clusterExport. This would be all the objects that you need in the child sessions.\nPass expressions with clusterEvalQ. This would include loading R packages and other code into the other sessions.\nSet a seed (if you are doing something that involves randomness)\n\nDo your call: parApply, parLapply, etc.\nStop the cluster with clusterStop\n\nAs we mention later, step 2 will depend on the type of cluster you are using. If you are using a Socket connection (PSOCK cluster), then the spawned R sessions will be completely, fresh (no data or R packages pre-loaded); whereas using a Fork connection (FORK cluster) will copy the current R session, including all objects and loaded packages."
  },
  {
    "objectID": "parallel_pkg.html#types-of-clusters-psock",
    "href": "parallel_pkg.html#types-of-clusters-psock",
    "title": "The parallel R package",
    "section": "Types of clusters: PSOCK",
    "text": "Types of clusters: PSOCK\n\nCan be created with makePSOCKCluster\nCreates brand new R Sessions (so nothing is inherited from the master), e.g.\n# This creates a cluster with 4 R sessions\ncl &lt;- makePSOCKCluster(4)\nChild sessions are connected to the master session via Socket connections\nCan be created outside the current computer, i.e., across multiple computers!"
  },
  {
    "objectID": "parallel_pkg.html#types-of-clusters-fork",
    "href": "parallel_pkg.html#types-of-clusters-fork",
    "title": "The parallel R package",
    "section": "Types of clusters: Fork",
    "text": "Types of clusters: Fork\n\nFork Cluster makeForkCluster:\nUses OS Forking,\nCopies the current R session locally (so everything is inherited from the master up to that point).\nData is only duplicated if altered (need to double check when this happens!)\nNot available on Windows.\n\nOther types are available via the function makeCluster from the snow R package (Simple Network of Workstations). These include MPI (Message Passing Interface) clusters and Slurm (Socket) clusters."
  },
  {
    "objectID": "parallel_pkg.html#a-template-program",
    "href": "parallel_pkg.html#a-template-program",
    "title": "The parallel R package",
    "section": "A template program",
    "text": "A template program\nThe following code chunk shows a template for using the parallel package in R. You can copy this and comment the bits that you don’t need:\nlibrary(parallel)\n\n# 1. CREATING A CLUSTER ----------------\nnnodes &lt;- 4L # Could be less or more!\ncl &lt;- makePSOCKcluster(nnodes)\n\n# 2. PREPARING THE CLUSTER -------------\n\n# Mostly if using PSOCK\nclusterEvalQ(cl, {\n  library(...) # Loading the necesary packages\n  source(...) # Source additional scripts\n})\n\n# Always if you are doing random numbers\nclusterSetRNGStream(cl, 123)\n\n# 3. DO YOUR CALL ----------------------\nans &lt;- parLapply(\n  cl,\n  ... long list to iterate ...,\n  function(x) {\n    ...\n  },\n  ... further arguments ...\n  )\n\n# 4. STOP THE CLUSTER\nstopCluster(cl)\nGenerally, the ... long list to iterate ... will be a vector or another list that contains either data (e.g., individual datasets), a sequence of numbers (e.g., from 1 to 1000), a list of file paths (if you were processing files individually), or directly a short sequence with numbers from 1 to the number of nodes (least common application).\nWhen calling parLapply or parSapply (the parallel versions of lapply and sapply respectively), the function call will automatically split the iterations across nodes using the splitIndices function. Here is an example of what happens under the hood:\n\n# Distributing 9 iterations across two cores\n(n_iterations &lt;- parallel::splitIndices(nx = 9, ncl = 2))\n\n[[1]]\n[1] 1 2 3 4\n\n[[2]]\n[1] 5 6 7 8 9\n\n\nWhich means that the first R session will get 4 jobs, wereas the second R session will get 5 jobs. This way, each spawned R session (child session) gets to do a similiar number of iterations."
  },
  {
    "objectID": "parallel_pkg.html#example-running-a-linear-regression-across-multiple-columns",
    "href": "parallel_pkg.html#example-running-a-linear-regression-across-multiple-columns",
    "title": "The parallel R package",
    "section": "Example: Running a linear regression across multiple columns",
    "text": "Example: Running a linear regression across multiple columns\nIn genomics, it is common to analyze genomic data at the gene level comparing expression levels against some phenotype/disease. A simple analysis consists of running a linear regression across multiple columns (genes) of a data frame. The following code-block generates some artificial data we can use for this example:\n\nset.seed(331)\nn_genes &lt;- 10000\nn_obs &lt;- 1000\n\n# A random matrix of omics\nX_genes &lt;- rnorm(n_obs * n_genes) |&gt;\n  matrix(nrow = n_obs)\n\n# A random phenotype (completely unrelated for this example)\nY &lt;- rnorm(n_obs) |&gt; cbind()\n\nWe will wrap the analysis into a function so we can do benchmarking. We will use the lapply function to iterate over the columns of X_genes\n\nols_serial &lt;- function(X, Y) {\n  lapply(\n    X = seq_len(n_genes),\n    FUN = \\(i) {lm.fit(X[, i, drop = FALSE], Y) |&gt; coef()}\n  ) |&gt; do.call(what = rbind)\n}\n\n# Calling the function and looking at the first few rows\nols_serial(X_genes, Y) |&gt; head()\n\n               x1\n[1,]  0.029403088\n[2,]  0.008907854\n[3,] -0.027246099\n[4,] -0.031280262\n[5,] -0.001309752\n[6,]  0.066971469\n\n\n\n\n\n\n\n\nTip\n\n\n\nLike we did in the efficient programming section, instead of using lm() or glm(), we can use lm.fit() for better performance. The lm.fit() function does less than the lm() function by skipping computing residuals and other overhead, making it faster for large datasets.\n\n\nUsing parallel computing (and following the template we presented earlier), this could be done in the following way with the parallel package:\n\nlibrary(parallel)\n\nols_parallel &lt;- function(X, Y, ncores) {\n  # 1. CREATING A CLUSTER ----------------\n  cl &lt;- makePSOCKcluster(ncores)\n  \n  # This will be called when exiting the function\n  on.exit(stopCluster(cl)) \n\n  # 2. PREPARING THE CLUSTER -------------\n  # We copy the data over\n  clusterExport(cl, c(\"X\", \"Y\"), envir = environment())\n\n  # 3. DO YOUR CALL ----------------------\n  parLapply(\n    cl,\n    seq_len(n_genes),\n    function(i) {\n      lm.fit(X[, i, drop = FALSE], Y) |&gt; coef()\n    }\n  ) |&gt; do.call(what = rbind)\n}\n\n# Checking it works\nols_parallel(X_genes, Y, ncores = 4L) |&gt; head()\n\n               x1\n[1,]  0.029403088\n[2,]  0.008907854\n[3,] -0.027246099\n[4,] -0.031280262\n[5,] -0.001309752\n[6,]  0.066971469\n\n\n\n\n\n\n\n\nTip\n\n\n\nJust like the return(), on.exit() can only be used within a function call. We could have used stopCluster(cl) at the end as we do in our template example, but the benefit of using on.exit() is that it will be called automatically when the function exits, even if an error occurs. This helps to ensure that the cluster is always stopped properly.\n\n\nNow that we have the function implemented, we can go ahead and (1) compare results and (2) measure performance.\n\nlibrary(microbenchmark)\n\nmicrobenchmark(\n  serial = ols_serial(X_genes, Y),\n  parallel = ols_parallel(X_genes, Y, ncores = 4L),\n  times = 10L,\n  check = \"identical\"\n)\n\nWarning in microbenchmark(serial = ols_serial(X_genes, Y), parallel =\nols_parallel(X_genes, : less accurate nanosecond times to avoid potential\ninteger overflows\n\n\nUnit: milliseconds\n     expr      min        lq      mean    median        uq       max neval\n   serial  364.859  377.0944  400.9552  399.6909  417.0765  472.7684    10\n parallel 1239.990 1322.1260 1401.6740 1436.9526 1471.8845 1531.0783    10\n\n\nFrom the comparison, we can see that the parallel version is significantly slower than the serial version. Two things to note here are (a) the task we are running is already fast (about 0.3 seconds on average for the serial run) and (b) there is an overhead cost associated with creating, preparing, and stopping the cluster. As we mentioned earlier, parallel optimizations only make sense if your code is already talking a significant amount of time, making the overhead cost associated with the setup relatively small. The following implementation of the function should make it significantly faster:\n\nols_parallel2 &lt;- function(cl) {\n  # 1. CREATING A CLUSTER ----------------\n  # 2. PREPARING THE CLUSTER -------------\n  # No need anymore as we are handling the core outside\n\n  # 3. DO YOUR CALL ----------------------\n  parLapply(\n    cl,\n    seq_len(n_genes),\n    function(i) {\n      lm.fit(X_genes[, i, drop = FALSE], Y) |&gt; coef()\n    }\n  ) |&gt; do.call(what = rbind)\n}\n\n# Checking it works\ncl &lt;- makePSOCKcluster(4)\nclusterExport(cl, c(\"X_genes\", \"Y\"))\nols_parallel2(cl) |&gt; head()\n\n               x1\n[1,]  0.029403088\n[2,]  0.008907854\n[3,] -0.027246099\n[4,] -0.031280262\n[5,] -0.001309752\n[6,]  0.066971469\n\n# 4. STOP THE CLUSTER\nstopCluster(cl)\n\nThe main differences from the previous version of the function are:\n\nWe are creating the cluster outside of the function and passing it as an argument.\nWe are exporting the X_genes and Y variables to the cluster only once, which should also reduce overhead significantly.\nBecause of the previous step, we are now calling X_genes directly in the main function.\nThe cluster is stopped outside of the function call (since the function no longer manages the cluster object).\n\nLet’s measure the performance to see how much faster the parallel version is.\n\nlibrary(microbenchmark)\n\n# We need to prepare the cluster before hand\ncl &lt;- makePSOCKcluster(4)\nclusterExport(cl, c(\"X_genes\", \"Y\"))\n\nmicrobenchmark(\n  serial = ols_serial(X_genes, Y),\n  parallel = ols_parallel2(cl),\n  times = 10L,\n  check = \"identical\"\n)\n\nUnit: milliseconds\n     expr       min       lq     mean   median       uq      max neval\n   serial 339.13429 343.8196 376.7939 363.3572 393.6694 475.9597    10\n parallel  96.69506  98.3918 110.6199 107.1158 118.9023 135.3168    10\n\n# We need to stop the cluster\nstopCluster(cl)\n\nNow, the parallel version is significantly faster than the serial version. Just using the parallel package (or any other package that can be used for parallel computing) does not guarantee improved performance."
  },
  {
    "objectID": "parallel_pkg.html#more-examples",
    "href": "parallel_pkg.html#more-examples",
    "title": "The parallel R package",
    "section": "More examples",
    "text": "More examples\nThe following three examples are a simple application of the package in which we are explicitly running as many replications as threads the cluster has. Generally, the number of replicates will be a function of the data.\n\nEx 1: Parallel RNG with makePSOCKCluster\n\n\n\n\n\n\nCaution\n\n\n\nUsing more threads than cores available on your computer is never a good idea. As a rule of thumb, clusters should be created using parallel::detectCores() - 1 cores (so you leave one free for the rest of your computer.)\n\n\n\n# 1. CREATING A CLUSTER\nlibrary(parallel)\nnnodes &lt;- 4L\ncl     &lt;- makePSOCKcluster(nnodes)    \n# 2. PREPARING THE CLUSTER\nclusterSetRNGStream(cl, 123) # Equivalent to `set.seed(123)`\n# 3. DO YOUR CALL\nans &lt;- parSapply(cl, 1:nnodes, function(x) runif(1e3))\n(ans0 &lt;- var(ans))\n\n              [,1]          [,2]          [,3]          [,4]\n[1,]  0.0861888293 -0.0001633431  5.939143e-04 -3.672845e-04\n[2,] -0.0001633431  0.0853841838  2.390790e-03 -1.462154e-04\n[3,]  0.0005939143  0.0023907904  8.114219e-02 -4.714618e-06\n[4,] -0.0003672845 -0.0001462154 -4.714618e-06  8.467722e-02\n\n\nMaking sure it is reproducible\n\n# I want to get the same!\nclusterSetRNGStream(cl, 123)\nans1 &lt;- var(parSapply(cl, 1:nnodes, function(x) runif(1e3)))\n# 4. STOP THE CLUSTER\nstopCluster(cl)\nall.equal(ans0, ans1) # All equal!\n\n[1] TRUE\n\n\n\n\nEx 2: Parallel RNG with makeForkCluster\nIn the case of makeForkCluster\n\n# 1. CREATING A CLUSTER\nlibrary(parallel)\n# The fork cluster will copy the -nsims- object\nnsims  &lt;- 1e3\nnnodes &lt;- 4L\ncl     &lt;- makeForkCluster(nnodes)    \n# 2. PREPARING THE CLUSTER\nclusterSetRNGStream(cl, 123)\n# 3. DO YOUR CALL\nans &lt;- do.call(cbind, parLapply(cl, 1:nnodes, function(x) {\n  runif(nsims) # Look! we use the nsims object!\n               # This would have fail in makePSOCKCluster\n               # if we didn't copy -nsims- first.\n  }))\n(ans0 &lt;- var(ans))\n\n              [,1]          [,2]          [,3]          [,4]\n[1,]  0.0861888293 -0.0001633431  5.939143e-04 -3.672845e-04\n[2,] -0.0001633431  0.0853841838  2.390790e-03 -1.462154e-04\n[3,]  0.0005939143  0.0023907904  8.114219e-02 -4.714618e-06\n[4,] -0.0003672845 -0.0001462154 -4.714618e-06  8.467722e-02\n\n\nAgain, we want to make sure this is reproducible\n\n# Same sequence with same seed\nclusterSetRNGStream(cl, 123)\nans1 &lt;- var(do.call(cbind, parLapply(cl, 1:nnodes, function(x) runif(nsims))))\nans0 - ans1 # A matrix of zeros\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n[4,]    0    0    0    0\n\n# 4. STOP THE CLUSTER\nstopCluster(cl)\n\nWell, if you are a Mac-OS/Linux user, there’s a more straightforward way of doing this…\n\n\nEx 3: Parallel RNG with mclapply (Forking on the fly)\nIn the case of mclapply, the forking (cluster creation) is done on the fly!\n\n# 1. CREATING A CLUSTER\nlibrary(parallel)\n# The fork cluster will copy the -nsims- object\nnsims  &lt;- 1e3\nnnodes &lt;- 4L\n# cl     &lt;- makeForkCluster(nnodes) # mclapply does it on the fly\n# 2. PREPARING THE CLUSTER\nset.seed(123) \n# 3. DO YOUR CALL\nans &lt;- do.call(cbind, mclapply(1:nnodes, function(x) runif(nsims)))\n(ans0 &lt;- var(ans))\n\n             [,1]          [,2]         [,3]          [,4]\n[1,] 0.0855343139  1.818837e-03 1.884180e-03  0.0001954201\n[2,] 0.0018188371  8.297611e-02 1.418663e-05 -0.0013661037\n[3,] 0.0018841803  1.418663e-05 8.044551e-02  0.0042199417\n[4,] 0.0001954201 -1.366104e-03 4.219942e-03  0.0825423038\n\n\nOnce more, we want to make sure this is reproducible\n\n# Same sequence with same seed\nset.seed(123) \nans1 &lt;- var(do.call(cbind, mclapply(1:nnodes, function(x) runif(nsims))))\nans0 - ans1 # A matrix of zeros\n\n             [,1]          [,2]         [,3]         [,4]\n[1,] -0.004179373  0.0011362932  0.004144171 3.573133e-03\n[2,]  0.001136293 -0.0040557568  0.001339580 8.442004e-04\n[3,]  0.004144171  0.0013395796 -0.002173317 3.433620e-03\n[4,]  0.003573133  0.0008442004  0.003433620 4.073255e-05\n\n# 4. STOP THE CLUSTER\n# stopCluster(cl) no need of doing this anymore"
  },
  {
    "objectID": "parallel_pkg.html#exercise-overhead-costs",
    "href": "parallel_pkg.html#exercise-overhead-costs",
    "title": "The parallel R package",
    "section": "Exercise: Overhead costs",
    "text": "Exercise: Overhead costs\nCompare the timing of taking the sum of 100 numbers when parallelized versus not. For the unparallized (serialized) version, use the following:\n\nset.seed(123)\nx &lt;- runif(n=100)\n\nserial_sum &lt;- function(x){\n  x_sum &lt;- sum(x)\n  return(x_sum)\n}\n\nFor the parallized version, follow this outline\n\nlibrary(parallel)\n\n\nset.seed(123)\nx &lt;- runif(n=100)\n\nparallel_sum &lt;- function(){\n  \n  \n  # Set number of cores to use\n  # make cluster and export to the cluster the x variable\n  # Use \"split function to divide x up into as many chunks as the number of cores\n  \n  # Calculate partial sums doing something like:\n  \n  partial_sums &lt;- parallel::parSapply(cl, x_split, sum)\n  \n  # Stop the cluster\n  \n  # Add and return the partial sums\n  \n}\n\nCompare the timing of the two approaches:\n\nmicrobenchmark::microbenchmark(\n  serial   = serial_sum(x),\n  parallel = parallel_sum(x),\n  times    = 10,\n  unit     = \"relative\"\n)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Attendees during the morning session led by Andrew Redd (Photo by George G Vega Yon)\nForeSITE"
  },
  {
    "objectID": "index.html#about-the-workshop",
    "href": "index.html#about-the-workshop",
    "title": "Welcome!",
    "section": "About the workshop",
    "text": "About the workshop\nEfficient programming is crucial for working with large datasets and complex analyses in R. This half-day workshop is designed to teach practical techniques for improving R code performance through vectorization, parallel computing, and specialized packages like data.table. Participants will learn how to identify bottlenecks in their code and apply appropriate optimization strategies to dramatically improve performance.\n\nTime and location\nThe workshop will take place on Friday August 15th, 2025 at 9am-1pm MT. More details will be provided to registered attendees.\n\n\nWhat are good use cases for efficient R programming?\n\nYou work with large datasets that take a long time to process\nYour R scripts run slowly and you need to optimize performance\nYou want to learn about vectorization techniques to replace slow loops\nYou’re interested in parallel computing to utilize multiple CPU cores\nYou want to master data.table for fast data manipulation\nYou need to process data that doesn’t fit in memory efficiently\nYou want to benchmark and profile your R code to identify bottlenecks\n\n\n\nWho should attend?\nR programming users with intermediate experience who want to improve their code’s performance.\n\n\nWhat level of programming should attendees have?\nAttendees should have solid experience using the R programming language, including writing functions, working with data frames, and basic data manipulation using tools like dplyr, base R, or data.table. Some familiarity with loops and apply functions would be helpful.\n\n\nWhat tools will be used during the workshop?\nBesides the R programming language, we will be using RStudio. Participants should have R and RStudio installed on their laptops. We will also cover specialized packages including:\n\ndata.table for fast data manipulation\nparallel for parallel computing\nmicrobenchmark and profvis for performance measurement"
  },
  {
    "objectID": "index.html#registration",
    "href": "index.html#registration",
    "title": "Welcome!",
    "section": "Registration",
    "text": "Registration\nFor registration, please reach out to your ForeSITE contact. You can also email us at george.vegayon@utah.edu."
  },
  {
    "objectID": "index.html#funding",
    "href": "index.html#funding",
    "title": "Welcome!",
    "section": "Funding",
    "text": "Funding\nThis workshop is organized by ForeSITE using funds from the CDC’s Center for Forecasting and Outbreak Analytics (CFA) (Award number 1U01CK000585; 75D30121F00003)."
  },
  {
    "objectID": "index.html#ai-disclaimer",
    "href": "index.html#ai-disclaimer",
    "title": "Welcome!",
    "section": "AI Disclaimer",
    "text": "AI Disclaimer\nThis project contains AI-generated content. Particularly, via assistance (code completion and suggestions) using GitHub Copilot."
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "Program",
    "section": "",
    "text": "This half-day workshop will focus on practical techniques for writing efficient R code. Participants will learn about vectorization, parallel computing, and specialized packages to dramatically improve their R code performance. The session will include hands-on exercises and real-world examples.\nForeSITE"
  },
  {
    "objectID": "program.html#plan",
    "href": "program.html#plan",
    "title": "Program",
    "section": "Plan",
    "text": "Plan\nThe first two hours of the workshop will cover the following topics:\n\nProfiling (Profiling)\nWriting efficient code (Efficiency)\nThe data.table package (Data.Table)\nParallel computing in R (Parallel computing in R and the parallel package)\n\nThe last hour will be dedicated to looking into the participants’ specific use cases and challenges, providing tailored advice and solutions. Much of the content will be driven by the participants’ needs and questions, and due to the time constraints, we will most likely skip some sections of the lectures."
  },
  {
    "objectID": "program.html#resources-and-git-materials",
    "href": "program.html#resources-and-git-materials",
    "title": "Program",
    "section": "Resources and Git Materials",
    "text": "Resources and Git Materials\nThe workshop will utilize materials from the git folder for version control best practices when working with performance-critical code. Additional resources and example datasets will be provided during the workshop."
  },
  {
    "objectID": "participants.html",
    "href": "participants.html",
    "title": "Participants",
    "section": "",
    "text": "Some of the participants of the workshop are listed below:\n\n\nGeorge G Vega Yon (pronouns: He/Him/Él) University of Utah   \n\nI am an Assistant Professor of Research at the Division of Epidemiology at the University of Utah. I work on studying Complex Systems using Statistical Computing. I have over ten years of experience developing scientific software focusing on high-performance computing, data visualization, and social network analysis. My training is in Public Policy (M.A. UAI, 2011), Economics (M.Sc. Caltech, 2015), and Biostatistics (Ph.D. USC, 2020).\n\n\n\n\nForeSITE"
  }
]