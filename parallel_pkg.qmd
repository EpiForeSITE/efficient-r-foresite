---
title: The parallel R package
author: George G. Vega Yon, Ph.D.
date: 2025-08-14
date-modified: 2025-08-18
mermaid: 
  theme: neutral
---

::: {.callout-note}
This content was originally published in the book "Applied HPC with R" by George G. Vega Yon, Ph.D. You can find the book at <https://book-hpc.ggvy.cl>
:::

Although R was not built for parallel computing, multiple ways of parallelizing your R code exist. One of these is the `parallel` package. This R package, shipped with base R, provides various functions to parallelize R code using [embarrassingly parallel computing](https://en.wikipedia.org/w/index.php?title=Embarrassingly_parallel&oldid=1136401514){target="_blank"}, i.e., a divide-and-conquer-type strategy. The basic idea is to start multiple R sessions (usually called child processes), connect the main session with those, and send them instructions. This section goes over a common workflow to work with R's parallel.

## Parallel workflow

```{mermaid}
flowchart LR
  start((Start)) --> one["`Create<br>a cluster`"]
  one --> two
  subgraph two[Prepare the session]
    direction TB
    copy[Copy objects]~~~eval[Evaluate<br>expressions]
    eval~~~seed[Set seed]
  end

  two --> three[Do your<br>call]
  three --> four[Stop the<br>cluster]
```

(Usually) We do the following:

1.  Create a `PSOCK/FORK` (or other) cluster using `makePSOCKCluster`/`makeForkCluster` (or `makeCluster`). How many child processes will depend on how many threads your computer has. A rule of thumb is to use `parallel::detectCores() - 1` cores (so you leave one free for the rest of your computer).

2.  Copy/prepare each R session (if you are using a `PSOCK` cluster):

    a.  Copy objects with `clusterExport`. This would be all the objects that you need in the child sessions.

    b.  Pass expressions with `clusterEvalQ`. This would include loading R packages and other code into the other sessions.

    c.  Set a seed (if you are doing something that involves randomness)

3.  Do your call: `parApply`, `parLapply`, etc. 

4.  Stop the cluster with `clusterStop`

As we mention later, step 2 will depend on the type of cluster you are using. If you are using a Socket connection (`PSOCK` cluster), then the spawned R sessions will be completely, fresh (no data or R packages pre-loaded); whereas using a Fork connection (`FORK` cluster) will copy the current R session, including all objects and loaded packages.

## Types of clusters: PSOCK

-   Can be created with `makePSOCKCluster`

-   Creates brand new R Sessions (so nothing is inherited from the master), e.g.
    
    ```r
    # This creates a cluster with 4 R sessions
    cl <- makePSOCKCluster(4)
    ```

-   Child sessions are connected to the master session via Socket connections

-   Can be created outside the current computer, **i.e.**, across multiple computers!

## Types of clusters: Fork

-   Fork Cluster `makeForkCluster`:

-   Uses OS [Forking](https://en.wikipedia.org/wiki/Fork_(system_call)),

-   Copies the current R session locally (so everything is inherited from
    the master up to that point).
    
-   Data is only duplicated if altered (need to double check when this happens!)

-   Not available on Windows.

Other types are available via the function `makeCluster` from the [**snow**](https://cran.r-project.org/package=snow) R package
(Simple Network of Workstations). These include MPI (Message Passing Interface) clusters and Slurm (Socket) clusters.


## A template program

The following code chunk shows a template for using the `parallel` package in R. You can copy this and comment the bits that you don't need:

```r
library(parallel)

# 1. CREATING A CLUSTER ----------------
nnodes <- 4L # Could be less or more!
cl <- makePSOCKcluster(nnodes)

# 2. PREPARING THE CLUSTER -------------

# Mostly if using PSOCK
clusterEvalQ(cl, {
  library(...) # Loading the necesary packages
  source(...) # Source additional scripts
})

# Always if you are doing random numbers
clusterSetRNGStream(cl, 123)

# 3. DO YOUR CALL ----------------------
ans <- parLapply(
  cl,
  ... long list to iterate ...,
  function(x) {
    ...
  },
  ... further arguments ...
  )

# 4. STOP THE CLUSTER
stopCluster(cl)
```

Generally, the `... long list to iterate ...` will be a vector or another list that contains either data (e.g., individual datasets), a sequence of numbers (e.g., from 1 to 1000), a list of file paths (if you were processing files individually), or directly a short sequence with numbers from 1 to the number of nodes (least common application).

When calling `parLapply` or `parSapply` (the parallel versions of `lapply` and `sapply` respectively), the function call will automatically split the iterations across nodes using the `splitIndices` function. Here is an example of what happens under the hood:

```{r}
#| label: cluster-split
# Distributing 9 iterations across two cores
(n_iterations <- parallel::splitIndices(nx = 9, ncl = 2))
```

Which means that the first R session will get `r length(n_iterations[[1]])` jobs, wereas the second R session will get `r length(n_iterations[[2]])` jobs. This way, each spawned R session (child session) gets to do a similiar number of iterations.

## Example: Running a linear regression across multiple columns

In genomics, it is common to analyze genomic data at the gene level comparing expression levels against some phenotype/disease. A simple analysis consists of running a linear regression across multiple columns (genes) of a data frame. The following code-block generates some artificial data we can use for this example:

```{r}
#| label: omics-data
set.seed(331)
n_genes <- 10000
n_obs <- 1000

# A random matrix of omics
X_genes <- rnorm(n_obs * n_genes) |>
  matrix(nrow = n_obs)

# A random phenotype (completely unrelated for this example)
Y <- rnorm(n_obs) |> cbind()
```

We will wrap the analysis into a function so we can do benchmarking. We will use the `lapply` function to iterate over the columns of `X_genes`

```{r}
#| label: omics-lapply
ols_serial <- function(X, Y) {
  lapply(
    X = seq_len(n_genes),
    FUN = \(i) {lm.fit(X[, i, drop = FALSE], Y) |> coef()}
  ) |> do.call(what = rbind)
}

# Calling the function and looking at the first few rows
ols_serial(X_genes, Y) |> head()
```

::: {.callout-tip}
Like we did in the efficient programming section, instead of using `lm()` or `glm()`, we can use `lm.fit()` for better performance. The `lm.fit()` function does less than the `lm()` function by skipping computing residuals and other overhead, making it faster for large datasets.
:::

Using parallel computing (and following the template we presented earlier), this could be done in the following way with the parallel package:

```{r}
#| label: omics-parlapply
library(parallel)

ols_parallel <- function(X, Y, ncores) {
  # 1. CREATING A CLUSTER ----------------
  cl <- makePSOCKcluster(ncores)
  
  # This will be called when exiting the function
  on.exit(stopCluster(cl)) 

  # 2. PREPARING THE CLUSTER -------------
  # We copy the data over
  clusterExport(cl, c("X", "Y"), envir = environment())

  # 3. DO YOUR CALL ----------------------
  parLapply(
    cl,
    seq_len(n_genes),
    function(i) {
      lm.fit(X[, i, drop = FALSE], Y) |> coef()
    }
  ) |> do.call(what = rbind)
}

# Checking it works
ols_parallel(X_genes, Y, ncores = 4L) |> head()
```

::: {.callout-tip}
Just like the `return()`, `on.exit()` can only be used within a function call. We could have used `stopCluster(cl)` at the end as we do in our template example, but the benefit of using `on.exit()` is that it will be called automatically when the function exits, even if an error occurs. This helps to ensure that the cluster is always stopped properly.
:::

Now that we have the function implemented, we can go ahead and (1) compare results and (2) measure performance.

```{r}
#| label: omics-bench1
#| cache: true
library(microbenchmark)

microbenchmark(
  serial = ols_serial(X_genes, Y),
  parallel = ols_parallel(X_genes, Y, ncores = 4L),
  times = 10L,
  check = "identical"
)
```

From the comparison, we can see that the parallel version is significantly slower than the serial version. Two things to note here are (a) the task we are running is already fast (about 0.3 seconds on average for the serial run) and (b) there is an overhead cost associated with creating, preparing, and stopping the cluster. As we mentioned earlier, parallel optimizations only make sense if your code is already talking a significant amount of time, making the overhead cost associated with the setup relatively small. The following implementation of the function should make it significantly faster:

```{r}
#| label: omics-parlapply2
ols_parallel2 <- function(cl) {
  # 1. CREATING A CLUSTER ----------------
  # 2. PREPARING THE CLUSTER -------------
  # No need anymore as we are handling the core outside

  # 3. DO YOUR CALL ----------------------
  parLapply(
    cl,
    seq_len(n_genes),
    function(i) {
      lm.fit(X_genes[, i, drop = FALSE], Y) |> coef()
    }
  ) |> do.call(what = rbind)
}

# Checking it works
cl <- makePSOCKcluster(4)
clusterExport(cl, c("X_genes", "Y"))
ols_parallel2(cl) |> head()

# 4. STOP THE CLUSTER
stopCluster(cl)
```

The main differences from the previous version of the function are:

1. We are creating the cluster outside of the function and passing it as an argument.

2. We are exporting the `X_genes` and `Y` variables to the cluster only once, which should also reduce overhead significantly.

3. Because of the previous step, we are now calling `X_genes` directly in the main function.

4. The cluster is stopped outside of the function call (since the function no longer manages the cluster object).


Let's measure the performance to see how much faster the parallel version is.

```{r}
#| label: omics-bench2
#| cache: true
library(microbenchmark)

# We need to prepare the cluster before hand
cl <- makePSOCKcluster(4)
clusterExport(cl, c("X_genes", "Y"))

microbenchmark(
  serial = ols_serial(X_genes, Y),
  parallel = ols_parallel2(cl),
  times = 10L,
  check = "identical"
)

# We need to stop the cluster
stopCluster(cl)
```

Now, the parallel version is significantly faster than the serial version. Just using the parallel package (or any other package that can be used for parallel computing) does not guarantee improved performance.

## More examples

The following three examples are a simple application of the package in which we are explicitly running as many replications as threads the cluster has. Generally, the number of replicates will be a function of the data.

### Ex 1: Parallel RNG with `makePSOCKCluster`

::: {.callout-caution}
Using more threads than cores available on your computer is never a good idea. As a rule of thumb, clusters should be created using `parallel::detectCores() - 1` cores (so you leave one free for the rest of your computer.)
:::

```{r parallel-ex-psock, echo=TRUE}
# 1. CREATING A CLUSTER
library(parallel)
nnodes <- 4L
cl     <- makePSOCKcluster(nnodes)    
# 2. PREPARING THE CLUSTER
clusterSetRNGStream(cl, 123) # Equivalent to `set.seed(123)`
# 3. DO YOUR CALL
ans <- parSapply(cl, 1:nnodes, function(x) runif(1e3))
(ans0 <- var(ans))
```

Making sure it is reproducible

```{r parallel-ex-psock-cont, echo=TRUE}
# I want to get the same!
clusterSetRNGStream(cl, 123)
ans1 <- var(parSapply(cl, 1:nnodes, function(x) runif(1e3)))
# 4. STOP THE CLUSTER
stopCluster(cl)
all.equal(ans0, ans1) # All equal!
```

### Ex 2: Parallel RNG with `makeForkCluster`

In the case of `makeForkCluster`

```{r parallel-ex-fork, echo=TRUE, eval = TRUE}
# 1. CREATING A CLUSTER
library(parallel)
# The fork cluster will copy the -nsims- object
nsims  <- 1e3
nnodes <- 4L
cl     <- makeForkCluster(nnodes)    
# 2. PREPARING THE CLUSTER
clusterSetRNGStream(cl, 123)
# 3. DO YOUR CALL
ans <- do.call(cbind, parLapply(cl, 1:nnodes, function(x) {
  runif(nsims) # Look! we use the nsims object!
               # This would have fail in makePSOCKCluster
               # if we didn't copy -nsims- first.
  }))
(ans0 <- var(ans))
```

Again, we want to make sure this is reproducible

```{r parallel-ex-fork-cont, echo=TRUE}
# Same sequence with same seed
clusterSetRNGStream(cl, 123)
ans1 <- var(do.call(cbind, parLapply(cl, 1:nnodes, function(x) runif(nsims))))
ans0 - ans1 # A matrix of zeros
# 4. STOP THE CLUSTER
stopCluster(cl)
```

<text style="color:white;">Well, if you are a Mac-OS/Linux user, there's a more straightforward way of doing this...</text>


### Ex 3: Parallel RNG with `mclapply` (Forking on the fly)

In the case of `mclapply`, the forking (cluster creation) is done on the fly!

```{r parallel-ex-mclapply, echo=TRUE, eval = TRUE}
# 1. CREATING A CLUSTER
library(parallel)
# The fork cluster will copy the -nsims- object
nsims  <- 1e3
nnodes <- 4L
# cl     <- makeForkCluster(nnodes) # mclapply does it on the fly
# 2. PREPARING THE CLUSTER
set.seed(123) 
# 3. DO YOUR CALL
ans <- do.call(cbind, mclapply(1:nnodes, function(x) runif(nsims)))
(ans0 <- var(ans))
```

Once more, we want to make sure this is reproducible

```{r parallel-ex-mclapply-cont, echo=TRUE}
# Same sequence with same seed
set.seed(123) 
ans1 <- var(do.call(cbind, mclapply(1:nnodes, function(x) runif(nsims))))
ans0 - ans1 # A matrix of zeros
# 4. STOP THE CLUSTER
# stopCluster(cl) no need of doing this anymore
```

## Exercise: Overhead costs

Compare the timing of taking the sum of 100 numbers when parallelized versus not.  For the unparallized (serialized) version, use the following:

```{r}
set.seed(123)
x <- runif(n=100)

serial_sum <- function(x){
  x_sum <- sum(x)
  return(x_sum)
}
```


For the parallized version, follow this outline

```{r}
library(parallel)
```

```{r, eval=FALSE}

set.seed(123)
x <- runif(n=100)

parallel_sum <- function(){
  
  
  # Set number of cores to use
  # make cluster and export to the cluster the x variable
  # Use "split function to divide x up into as many chunks as the number of cores
  
  # Calculate partial sums doing something like:
  
  partial_sums <- parallel::parSapply(cl, x_split, sum)
  
  # Stop the cluster
  
  # Add and return the partial sums
  
}

```


```{r, echo=FALSE, eval=TRUE, results=FALSE}

set.seed(123)
x <- runif(n=100)

parallel_sum <- function(x){
  
  
  num_cores <- 4L
  cl <- makeCluster(num_cores)
  clusterExport(cl,"x")
  
  x_split <- suppressWarnings(split(x, 1:num_cores))
  
  partial_sums <- parallel::parSapply(cl, x_split, sum)
  
  stopCluster(cl)
  
  x_sum <- sum(partial_sums)
  
  return(x_sum)
  
}
```

Compare the timing of the two approaches:

```{r}
#| label: overhead
#| eval: false
microbenchmark::microbenchmark(
  serial   = serial_sum(x),
  parallel = parallel_sum(x),
  times    = 10,
  unit     = "relative"
)
```
