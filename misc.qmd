---
title: Using data.table
---

For most cases, the most proper data wrangling tool in R is the `dplyr` package. Nonetheless, when dealing with large amounts of data, the `data.table` package is the fastest alternative available for doing data processing within R (see the [benchmarks](https://duckdblabs.github.io/db-benchmark/){target="_blank"}).

## Reading and Writing Data

Reading and writing operations with `data.table`'s `fread` and `fwrite` are highly optimized. Here is a benchmark we can do on our own:

```{r}
#| label: prepare-data
library(readr) # tidyverse's
library(data.table)
library(microbenchmark)

# Generating a large dataset of random numbers
set.seed(1231)
x <- runif(5e4 * 10) |> matrix(ncol = 10) |> data.frame()

# Creating tempfiles
temp_dt <- tempfile(fileext = ".csv")
temp_tv <- tempfile(fileext = ".csv")
temp_r  <- tempfile(fileext = ".csv")
```

```{r}
#| label: read-write
#| cache: true
bm <- microbenchmark(
  readr      = write_csv(x, temp_tv, num_threads = 1L, progress = FALSE),
  data.table = fwrite(
    x, temp_dt, verbose = FALSE, nThread = 1L,
    showProgress = FALSE),
  base       = write.csv(x, temp_r),
  times = 20
)

bm

# We can also visualize it
bm |>
  plot(
    log = "y",
    ylab = "Time (ms) (log10-scale)",
    main = "CSV Writing Benchmark"
  )
```

The same thing applies when reading data

```{r}
#| label: read-benchmark
bm <- microbenchmark(
  readr      = read_csv(
    temp_tv, progress = FALSE, num_threads = 1L,
    show_col_types = FALSE
    ),
  data.table = fread(
    temp_dt, verbose = FALSE, nThread = 1L,
    showProgress = FALSE
    ),
  base       = read.csv(temp_r),
  times = 10
)

bm

bm |>
  plot(
    log = "y",
    ylab = "Time (ms) (log10-scale)",
    main = "CSV Reading Benchmark"
  )
```

Under the hood, the `readr` package uses the [`vroom`](https://vroom.r-lib.org/){target="_blank"} package. Nonetheless, there are some operations (dealing with character data mostly), where the `vroom` package shines. Regardless, `data.table` is a perfect alternative as it goes beyond just reading/writing data.

## Example data manipulation

`data.table` is also the fastest for data manipulation. Here are a couple of examples aggregating data with data table vs dplyr

```{r}
#| label: get-data
input <- if (file.exists("flights14.csv")) {
   "flights14.csv"
} else {
  "https://raw.githubusercontent.com/Rdatatable/data.table/master/vignettes/flights14.csv"
}

# Reading the data
flights_dt <- fread(input)
flights_tb <- read_csv(input, show_col_types = FALSE)
```


```{r}
#| label: comparing
library(dplyr)

# To avoid some messaging from the function
options(dplyr.summarise.inform = FALSE)

microbenchmark(
  data.table = flights_dt[, .N, by = .(origin, dest)],
  dplyr = flights_tb |>
    group_by(origin, dest) |>
    summarise(n = n())
)
```

